#!rnn.py


import numpy as np

backend = "torch"
batch_size = 2400000
batching = "random"
cache_size = "0"
cleanup_old_models = True
debug_print_layer_output_template = True
dev = {
    "class": "MetaDataset",
    "datasets": {
        "features": {
            "partition_epoch": 1,
            "seq_ordering": "laplace:.1000",
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_core/returnn/hdf/BlissToPcmHDFJob.KErFrKsP3fTh/output/audio.hdf",
                "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_core/returnn/hdf/BlissToPcmHDFJob.Clwnntg2nopq/output/audio.hdf",
            ],
        },
        "targets": {
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_experiments/users/berger/recipe/returnn/hdf/BlissCorpusToTargetHdfJob.h7DH1ILPAElF/output/targets.hdf",
                "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_experiments/users/berger/recipe/returnn/hdf/BlissCorpusToTargetHdfJob.vESciFN5It1u/output/targets.hdf",
            ],
        },
    },
    "data_map": {"data": ("features", "data"), "targets": ("targets", "data")},
    "seq_order_control_dataset": "features",
}
device = "gpu"
extern_data = {"data": {"dim": 1}, "targets": {"dim": 79, "sparse": True}}
gradient_clip = 0.0
gradient_noise = 0.0
learning_rate_file = "learning_rates"
learning_rates = [
    4.1e-06,
    5.199999999999999e-06,
    6.299999999999999e-06,
    7.399999999999999e-06,
    8.499999999999998e-06,
    9.6e-06,
    1.07e-05,
    1.1799999999999999e-05,
    1.2899999999999998e-05,
    1.3999999999999998e-05,
    1.5099999999999998e-05,
    1.6199999999999997e-05,
    1.7299999999999997e-05,
    1.8399999999999997e-05,
    1.9499999999999996e-05,
    2.0599999999999996e-05,
    2.1699999999999996e-05,
    2.2799999999999995e-05,
    2.3899999999999995e-05,
    2.4999999999999994e-05,
    2.6099999999999994e-05,
    2.7199999999999994e-05,
    2.8299999999999993e-05,
    2.9399999999999996e-05,
    3.0499999999999996e-05,
    3.1599999999999996e-05,
    3.2699999999999995e-05,
    3.3799999999999995e-05,
    3.4899999999999995e-05,
    3.5999999999999994e-05,
    3.7099999999999994e-05,
    3.8199999999999993e-05,
    3.929999999999999e-05,
    4.039999999999999e-05,
    4.149999999999999e-05,
    4.259999999999999e-05,
    4.369999999999999e-05,
    4.479999999999999e-05,
    4.589999999999999e-05,
    4.699999999999999e-05,
    4.809999999999999e-05,
    4.919999999999999e-05,
    5.029999999999999e-05,
    5.139999999999999e-05,
    5.249999999999999e-05,
    5.359999999999999e-05,
    5.469999999999999e-05,
    5.5799999999999994e-05,
    5.6899999999999994e-05,
    5.7999999999999994e-05,
    5.909999999999999e-05,
    6.019999999999999e-05,
    6.13e-05,
    6.24e-05,
    6.35e-05,
    6.46e-05,
    6.57e-05,
    6.68e-05,
    6.79e-05,
    6.9e-05,
    7.01e-05,
    7.12e-05,
    7.23e-05,
    7.34e-05,
    7.45e-05,
    7.56e-05,
    7.67e-05,
    7.78e-05,
    7.89e-05,
    7.999999999999999e-05,
    8.109999999999999e-05,
    8.219999999999999e-05,
    8.329999999999999e-05,
    8.439999999999999e-05,
    8.549999999999999e-05,
    8.659999999999999e-05,
    8.769999999999999e-05,
    8.879999999999999e-05,
    8.989999999999999e-05,
    9.099999999999999e-05,
    9.209999999999999e-05,
    9.319999999999999e-05,
    9.429999999999999e-05,
    9.539999999999999e-05,
    9.649999999999999e-05,
    9.759999999999999e-05,
    9.869999999999999e-05,
    9.979999999999999e-05,
    0.00010089999999999999,
    0.00010199999999999999,
    0.00010309999999999999,
    0.00010419999999999998,
    0.00010529999999999998,
    0.00010639999999999998,
    0.00010749999999999998,
    0.0001086,
    0.0001097,
    0.0001108,
    0.0001119,
    0.000113,
    0.0001141,
    0.0001152,
    0.0001163,
    0.0001174,
    0.0001185,
    0.0001196,
    0.00012069999999999999,
    0.00012179999999999999,
    0.00012289999999999998,
    0.00012399999999999998,
    0.00012509999999999998,
    0.00012619999999999998,
    0.00012729999999999998,
    0.00012839999999999998,
    0.00012949999999999998,
    0.00013059999999999998,
    0.00013169999999999998,
    0.00013279999999999998,
    0.00013389999999999997,
    0.00013499999999999997,
    0.00013609999999999997,
    0.00013719999999999997,
    0.00013829999999999997,
    0.00013939999999999997,
    0.00014049999999999997,
    0.00014159999999999997,
    0.00014269999999999997,
    0.00014379999999999997,
    0.00014489999999999997,
    0.00014599999999999997,
    0.00014709999999999997,
    0.00014819999999999997,
    0.00014929999999999997,
    0.00015039999999999997,
    0.00015149999999999997,
    0.00015259999999999997,
    0.00015369999999999997,
    0.00015479999999999997,
    0.00015589999999999997,
    0.00015699999999999997,
    0.00015809999999999997,
    0.00015919999999999997,
    0.00016029999999999997,
    0.00016139999999999997,
    0.00016249999999999997,
    0.00016359999999999997,
    0.00016469999999999996,
    0.00016579999999999996,
    0.00016689999999999996,
    0.00016799999999999996,
    0.00016909999999999996,
    0.00017019999999999996,
    0.00017129999999999996,
    0.00017239999999999996,
    0.00017349999999999996,
    0.00017459999999999996,
    0.00017569999999999996,
    0.00017679999999999996,
    0.00017789999999999996,
    0.00017899999999999996,
    0.00018009999999999996,
    0.00018119999999999996,
    0.00018229999999999996,
    0.00018339999999999996,
    0.00018449999999999996,
    0.00018559999999999996,
    0.00018669999999999996,
    0.00018779999999999996,
    0.00018889999999999996,
    0.00018999999999999996,
    0.00019109999999999996,
    0.00019219999999999996,
    0.00019329999999999996,
    0.00019439999999999995,
    0.00019549999999999995,
    0.00019659999999999995,
    0.00019769999999999995,
    0.00019879999999999995,
    0.00019989999999999995,
    0.00020099999999999995,
    0.00020209999999999995,
    0.00020319999999999995,
    0.00020429999999999995,
    0.00020539999999999995,
    0.00020649999999999995,
    0.00020759999999999995,
    0.00020869999999999995,
    0.00020979999999999995,
    0.00021089999999999995,
    0.00021199999999999995,
    0.00021309999999999995,
    0.00021419999999999998,
    0.00021529999999999997,
    0.00021639999999999997,
    0.00021749999999999997,
    0.00021859999999999997,
    0.00021969999999999997,
    0.00022079999999999997,
    0.00022189999999999997,
    0.00022299999999999997,
    0.00022409999999999997,
    0.00022519999999999997,
    0.00022629999999999997,
    0.00022739999999999997,
    0.00022849999999999997,
    0.00022959999999999997,
    0.00023069999999999997,
    0.00023179999999999997,
    0.00023289999999999997,
    0.00023399999999999997,
    0.00023509999999999997,
    0.00023619999999999997,
    0.00023729999999999997,
    0.00023839999999999997,
    0.00023949999999999997,
    0.00024059999999999997,
    0.00024169999999999997,
    0.00024279999999999997,
    0.00024389999999999997,
    0.000245,
    0.00024609999999999996,
    0.0002472,
    0.00024829999999999996,
    0.0002494,
    0.00025049999999999996,
    0.0002516,
    0.00025269999999999996,
    0.0002538,
    0.00025489999999999996,
    0.000256,
    0.00025709999999999996,
    0.0002582,
    0.00025929999999999996,
    0.0002604,
    0.00026149999999999996,
    0.0002626,
    0.00026369999999999996,
    0.0002648,
    0.00026589999999999996,
    0.000267,
    0.00026809999999999996,
    0.0002692,
    0.00027029999999999996,
    0.0002714,
    0.00027249999999999996,
    0.0002736,
    0.00027469999999999996,
    0.0002758,
    0.00027689999999999995,
    0.000278,
    0.00027909999999999995,
    0.0002802,
    0.00028129999999999995,
    0.0002824,
    0.00028349999999999995,
    0.0002846,
    0.0002857,
    0.0002868,
    0.0002879,
    0.000289,
    0.0002901,
    0.0002912,
    0.0002923,
    0.0002934,
    0.0002945,
    0.0002956,
    0.0002967,
    0.0002978,
    0.0002989,
    0.0003,
    0.00029889999999999995,
    0.0002978,
    0.00029669999999999995,
    0.0002956,
    0.00029449999999999995,
    0.0002934,
    0.00029229999999999995,
    0.0002912,
    0.00029009999999999995,
    0.000289,
    0.00028789999999999995,
    0.0002868,
    0.00028569999999999995,
    0.0002846,
    0.00028349999999999995,
    0.0002824,
    0.00028129999999999995,
    0.0002802,
    0.00027909999999999995,
    0.000278,
    0.00027689999999999995,
    0.0002758,
    0.00027469999999999996,
    0.0002736,
    0.00027249999999999996,
    0.0002714,
    0.00027029999999999996,
    0.0002692,
    0.00026809999999999996,
    0.000267,
    0.00026589999999999996,
    0.0002648,
    0.00026369999999999996,
    0.0002626,
    0.00026149999999999996,
    0.0002604,
    0.00025929999999999996,
    0.0002582,
    0.00025709999999999996,
    0.000256,
    0.00025489999999999996,
    0.0002538,
    0.00025269999999999996,
    0.0002516,
    0.00025049999999999996,
    0.0002494,
    0.00024829999999999996,
    0.0002472,
    0.00024609999999999996,
    0.000245,
    0.00024389999999999997,
    0.0002428,
    0.00024169999999999997,
    0.0002406,
    0.00023949999999999997,
    0.0002384,
    0.00023729999999999997,
    0.0002362,
    0.00023509999999999997,
    0.000234,
    0.00023289999999999997,
    0.0002318,
    0.00023069999999999997,
    0.0002296,
    0.00022849999999999997,
    0.0002274,
    0.00022629999999999997,
    0.0002252,
    0.00022409999999999997,
    0.000223,
    0.00022189999999999997,
    0.0002208,
    0.00021969999999999997,
    0.0002186,
    0.00021749999999999997,
    0.0002164,
    0.00021529999999999997,
    0.0002142,
    0.00021309999999999998,
    0.000212,
    0.00021089999999999998,
    0.0002098,
    0.00020869999999999998,
    0.0002076,
    0.00020649999999999998,
    0.0002054,
    0.00020429999999999998,
    0.0002032,
    0.00020209999999999998,
    0.000201,
    0.00019989999999999998,
    0.0001988,
    0.00019769999999999998,
    0.0001966,
    0.00019549999999999998,
    0.00019439999999999998,
    0.00019329999999999998,
    0.00019219999999999998,
    0.00019109999999999998,
    0.00018999999999999998,
    0.00018889999999999998,
    0.00018779999999999998,
    0.00018669999999999998,
    0.00018559999999999998,
    0.00018449999999999999,
    0.00018339999999999999,
    0.00018229999999999999,
    0.00018119999999999999,
    0.0001801,
    0.000179,
    0.0001779,
    0.0001768,
    0.0001757,
    0.0001746,
    0.0001735,
    0.0001724,
    0.0001713,
    0.0001702,
    0.0001691,
    0.000168,
    0.0001669,
    0.0001658,
    0.0001647,
    0.0001636,
    0.0001625,
    0.0001614,
    0.0001603,
    0.0001592,
    0.0001581,
    0.000157,
    0.0001559,
    0.0001548,
    0.0001537,
    0.0001526,
    0.0001515,
    0.0001504,
    0.0001493,
    0.0001482,
    0.0001471,
    0.000146,
    0.0001449,
    0.0001438,
    0.0001427,
    0.0001416,
    0.0001405,
    0.0001394,
    0.0001383,
    0.0001372,
    0.0001361,
    0.000135,
    0.0001339,
    0.0001328,
    0.0001317,
    0.0001306,
    0.0001295,
    0.0001284,
    0.0001273,
    0.0001262,
    0.0001251,
    0.000124,
    0.0001229,
    0.0001218,
    0.0001207,
    0.00011960000000000001,
    0.00011850000000000001,
    0.00011740000000000001,
    0.00011630000000000001,
    0.00011520000000000001,
    0.00011410000000000001,
    0.00011300000000000001,
    0.00011190000000000001,
    0.00011080000000000001,
    0.00010970000000000001,
    0.00010860000000000001,
    0.00010750000000000001,
    0.00010640000000000001,
    0.00010530000000000001,
    0.00010420000000000001,
    0.00010310000000000001,
    0.00010200000000000001,
    0.00010090000000000001,
    9.980000000000001e-05,
    9.870000000000001e-05,
    9.760000000000001e-05,
    9.650000000000001e-05,
    9.540000000000001e-05,
    9.430000000000002e-05,
    9.320000000000002e-05,
    9.210000000000002e-05,
    9.100000000000002e-05,
    8.990000000000002e-05,
    8.879999999999999e-05,
    8.769999999999999e-05,
    8.659999999999999e-05,
    8.549999999999999e-05,
    8.439999999999999e-05,
    8.329999999999999e-05,
    8.219999999999999e-05,
    8.109999999999999e-05,
    7.999999999999999e-05,
    7.89e-05,
    7.78e-05,
    7.67e-05,
    7.56e-05,
    7.45e-05,
    7.34e-05,
    7.23e-05,
    7.12e-05,
    7.01e-05,
    6.9e-05,
    6.79e-05,
    6.68e-05,
    6.57e-05,
    6.46e-05,
    6.35e-05,
    6.24e-05,
    6.13e-05,
    6.02e-05,
    5.91e-05,
    5.8e-05,
    5.69e-05,
    5.58e-05,
    5.470000000000003e-05,
    5.36e-05,
    5.250000000000003e-05,
    5.14e-05,
    5.030000000000003e-05,
    4.92e-05,
    4.810000000000003e-05,
    4.7000000000000004e-05,
    4.590000000000003e-05,
    4.4800000000000005e-05,
    4.370000000000003e-05,
    4.2600000000000005e-05,
    4.150000000000003e-05,
    4.0400000000000006e-05,
    3.9300000000000034e-05,
    3.820000000000001e-05,
    3.7100000000000034e-05,
    3.600000000000001e-05,
    3.4900000000000035e-05,
    3.380000000000001e-05,
    3.2700000000000036e-05,
    3.160000000000001e-05,
    3.0500000000000037e-05,
    2.940000000000001e-05,
    2.8300000000000037e-05,
    2.720000000000001e-05,
    2.6100000000000038e-05,
    2.500000000000001e-05,
    2.390000000000004e-05,
    2.2800000000000012e-05,
    2.170000000000004e-05,
    2.0600000000000013e-05,
    1.950000000000004e-05,
    1.8400000000000014e-05,
    1.7299999999999987e-05,
    1.6200000000000014e-05,
    1.5099999999999988e-05,
    1.4000000000000015e-05,
    1.2899999999999988e-05,
    1.1800000000000016e-05,
    1.0699999999999989e-05,
    9.600000000000017e-06,
    8.49999999999999e-06,
    7.400000000000017e-06,
    6.2999999999999905e-06,
    5.200000000000018e-06,
    4.099999999999991e-06,
    2.9999999999999997e-06,
    2.9501666666666663e-06,
    2.900333333333333e-06,
    2.8504999999999996e-06,
    2.8006666666666663e-06,
    2.750833333333333e-06,
    2.7009999999999996e-06,
    2.6511666666666663e-06,
    2.601333333333333e-06,
    2.5514999999999996e-06,
    2.5016666666666663e-06,
    2.451833333333333e-06,
    2.4019999999999996e-06,
    2.3521666666666663e-06,
    2.302333333333333e-06,
    2.2524999999999996e-06,
    2.2026666666666663e-06,
    2.152833333333333e-06,
    2.1029999999999996e-06,
    2.0531666666666663e-06,
    2.0033333333333334e-06,
    1.9535e-06,
    1.9036666666666665e-06,
    1.8538333333333331e-06,
    1.8039999999999998e-06,
    1.7541666666666665e-06,
    1.7043333333333331e-06,
    1.6544999999999998e-06,
    1.6046666666666665e-06,
    1.5548333333333331e-06,
    1.5049999999999998e-06,
    1.4551666666666664e-06,
    1.4053333333333331e-06,
    1.3554999999999998e-06,
    1.3056666666666664e-06,
    1.255833333333333e-06,
    1.2059999999999998e-06,
    1.1561666666666664e-06,
    1.106333333333333e-06,
    1.0565e-06,
    1.0066666666666666e-06,
    9.568333333333333e-07,
    9.07e-07,
    8.571666666666666e-07,
    8.073333333333333e-07,
    7.575e-07,
    7.076666666666666e-07,
    6.578333333333333e-07,
    6.079999999999999e-07,
    5.581666666666666e-07,
    5.083333333333333e-07,
    4.5849999999999993e-07,
    4.086666666666666e-07,
    3.5883333333333326e-07,
    3.089999999999999e-07,
    2.591666666666666e-07,
    2.0933333333333325e-07,
    1.594999999999999e-07,
    1.0966666666666658e-07,
    5.983333333333324e-08,
    1e-08,
]
log = ["./returnn.log"]
log_batch_size = True
log_verbosity = 5
max_seqs = 60
model = "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_core/returnn/training/ReturnnTrainingJob.wdbAMfELLBp2/output/models/epoch"
num_epochs = 600
num_inputs = 1
num_outputs = {"targets": 79}
optimizer = {"class": "adamw", "epsilon": 1e-16, "weight_decay": 0.001}
save_interval = 1
target = "targets"
task = "train"
tf_log_memory_usage = True
train = {
    "class": "MetaDataset",
    "datasets": {
        "features": {
            "partition_epoch": 20,
            "seq_ordering": "laplace:.1000",
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_core/returnn/hdf/BlissToPcmHDFJob.VZM5dHZhqlnJ/output/audio.hdf"
            ],
        },
        "targets": {
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/work/i6_experiments/users/berger/recipe/returnn/hdf/BlissCorpusToTargetHdfJob.SYt8A5fOy2ta/output/targets.hdf"
            ],
        },
    },
    "data_map": {"data": ("features", "data"), "targets": ("targets", "data")},
    "seq_order_control_dataset": "features",
}
update_on_device = True
window = 1
config = {}

locals().update(**config)

import os
import sys

sys.path.insert(
    0, "/u/jxu/setups/librispeech-960/2023-10-17-torch-conformer-ctc/recipe"
)
from i6_experiments.users.jxu.experiments.ctc.lbs_960.pytorch_networks.baseline.conformer_ctc_d_model_512_num_layers_12_new_frontend_raw_wave_with_aux_loss import (
    ConformerCTCModel,
)
from i6_experiments.users.jxu.experiments.ctc.lbs_960.pytorch_networks.baseline.conformer_ctc_d_model_512_num_layers_12_new_frontend_raw_wave_with_aux_loss import (
    ConformerCTCConfig,
)
from i6_models.primitives.feature_extraction import LogMelFeatureExtractionV1Config
from i6_experiments.users.berger.pytorch.custom_parts.specaugment import (
    SpecaugmentByLengthConfigV1,
)
from i6_models.assemblies.conformer.conformer_v1 import ConformerEncoderV1Config
from i6_models.parts.frontend.vgg_act import VGG4LayerActFrontendV1Config
from torch.nn.modules.activation import ReLU
from i6_models.parts.frontend.vgg_act import VGG4LayerActFrontendV1
from i6_models.config import ModuleFactoryV1
from i6_models.assemblies.conformer.conformer_v1 import ConformerBlockV1Config
from i6_models.parts.conformer.feedforward import (
    ConformerPositionwiseFeedForwardV1Config,
)
from torch.nn.modules.activation import SiLU
from i6_models.parts.conformer.mhsa import ConformerMHSAV1Config
from i6_models.parts.conformer.convolution import ConformerConvolutionV1Config
from torch.nn.modules.activation import SiLU
from i6_models.parts.conformer.norm import LayerNormNC

cfg = ConformerCTCConfig(
    feature_extraction_cfg=LogMelFeatureExtractionV1Config(
        sample_rate=16000,
        win_size=0.025,
        hop_size=0.01,
        f_min=60,
        f_max=7600,
        min_amp=1e-10,
        num_filters=80,
        center=False,
        n_fft=400,
    ),
    specaugment_cfg=SpecaugmentByLengthConfigV1(
        time_min_num_masks=2,
        time_max_mask_per_n_frames=25,
        time_mask_max_size=20,
        freq_min_num_masks=2,
        freq_max_num_masks=5,
        freq_mask_max_size=8,
    ),
    conformer_cfg=ConformerEncoderV1Config(
        num_layers=12,
        frontend=ModuleFactoryV1(
            module_class=VGG4LayerActFrontendV1,
            cfg=VGG4LayerActFrontendV1Config(
                in_features=80,
                conv1_channels=32,
                conv2_channels=64,
                conv3_channels=64,
                conv4_channels=32,
                conv_kernel_size=(3, 3),
                conv_padding=None,
                pool1_kernel_size=(2, 1),
                pool1_stride=(2, 1),
                pool1_padding=None,
                pool2_kernel_size=(2, 1),
                pool2_stride=(2, 1),
                pool2_padding=None,
                activation=ReLU(),
                out_features=512,
            ),
        ),
        block_cfg=ConformerBlockV1Config(
            ff_cfg=ConformerPositionwiseFeedForwardV1Config(
                input_dim=512, hidden_dim=2048, dropout=0.1, activation=SiLU()
            ),
            mhsa_cfg=ConformerMHSAV1Config(
                input_dim=512, num_att_heads=8, att_weights_dropout=0.1, dropout=0.1
            ),
            conv_cfg=ConformerConvolutionV1Config(
                channels=512,
                kernel_size=31,
                dropout=0.1,
                activation=SiLU(),
                norm=LayerNormNC(512),
            ),
        ),
    ),
    target_size=79,
    aux_losses={"4": 0.3, "8": 0.3, "12": 1},
    recog_num_layer=12,
)
model_kwargs = {"cfg": cfg}


def get_model(epoch, step, **kwargs):
    return ConformerCTCModel(epoch=epoch, step=step, **model_kwargs, **kwargs)


from i6_experiments.users.jxu.experiments.ctc.lbs_960.pytorch_networks.baseline.conformer_ctc_d_model_512_num_layers_12_new_frontend_raw_wave_with_aux_loss import (
    train_step,
)
