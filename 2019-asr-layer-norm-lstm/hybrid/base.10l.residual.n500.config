#!crnn/rnn.py
# kate: syntax python;
# see also file:///u/zeyer/setups/quaero-en/training/quaero-train11/50h/ann/2015-07-29--lstm-gt50/config-train/dropout01.3l.n500.custom_lstm.adam.lr1e_3.config

import os
from subprocess import check_output

random_seed = 1

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode('utf-8')
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn

# data
num_inputs = 40  # Gammatone 40-dim
num_outputs = 9001 # 4501
EpochSplit = 6

def get_sprint_dataset(data):
    assert data in ["train", "cv"]
    epochSplit = {"train": EpochSplit, "cv": 1}

    # see /u/tuske/work/ASR/switchboard/corpus/readme
    # and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
    files = {}
    files["config"] = "config/training.config"
    files["corpus"] = "/u/tuske/work/ASR/switchboard/corpus/xml/train.corpus.gz"
    files["segments"] = "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000"}[data]
    files["features"] = "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.bundle"
    files["lexicon"] = "/u/tuske/work/ASR/switchboard/corpus/train.lex.v1_0_3.ci.gz"
    files["alignment"] = "dependencies/tuske__2016_01_28__align.combined.train"
    files["cart"] = "/u/tuske/work/ASR/switchboard/initalign/data/%s" % {9001: "cart-9000"}[num_outputs]
    for k, v in sorted(files.items()):
        assert os.path.exists(v), "%s %r does not exist" % (k, v)
    estimated_num_seqs = {"train": 227047, "cv": 3000}  # wc -l segment-file

    # features: /u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.*
    args = [
    "--config=" + files["config"],
    lambda: "--*.corpus.file=" + cf(files["corpus"]),
    lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
    {"train": "--*.corpus.segment-order-shuffle=true", "cv": "--*.corpus.segment-order-sort-by-time-length=true"}[data],
    "--*.state-tying.type=cart",
    lambda: "--*.state-tying.file=" + cf(files["cart"]),
    "--*.trainer-output-dimension=%i" % num_outputs,
    lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
    lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
    lambda: "--*.feature-cache-path=" + cf(files["features"]),
    #"--*.mean-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/mean",
    #"--*.variance-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/std",
    "--*.log-channel.file=log/crnn.sprint.train-dataset.xml",
    "--*.window-size=1",
    "--*.trainer-output-dimension=%i" % num_outputs
    ]
    return {
    "class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
    "sprintConfigStr": args,
    "input_stddev": 3.,
    "partitionEpoch": epochSplit[data],
    "estimated_num_seqs": estimated_num_seqs[data] // (epochSplit[data] or 1)}
train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")
sprint_interface_dataset_opts = {"input_stddev": 3.}
cache_size = "0"
window = 1

def add_lstm(source, unit, n_out=500, dropout=0.1, L2=0.01, **kwargs):
    d = {"class": "rec", 
         "unit": unit, 
         "n_out" : n_out, 
         "dropout": 0.1, 
         "L2": L2, 
         "from" : source}
    d.update(**kwargs)
    return d

def build_network(num_layers, unit):
    net = {}
    for i in range(num_layers):
        if i == 0:
            inp = "data"
        else:
            inp = "out%i" % (i-1)
        net["lstm%i_fw" % i] = add_lstm(source=[inp], unit=unit, direction=1)
        net["lstm%i_bw" % i] = add_lstm(source=[inp], unit=unit, direction=-1)
        if i > 0:
            net["concat%i" % i] = {"class": "copy", "from": ["lstm%i_fw" % i, "lstm%i_bw" % i]} # LSTM * 2
            net["out%i" % i] = {"class": "combine", 
                                "kind": "add", 
                                "from": ["concat%i" % i, inp]} # LSTM * 2
        else:
            net["out%i" % i] = {"class": "copy", "from": ["lstm%i_fw" % i, "lstm%i_bw" % i]}

    net["output"] = {"class" : "softmax", 
                     "loss" : "ce", 
                     "loss_opts": {"focal_loss_factor": 2.0}, 
                     "from" : ["out%i" % (num_layers-1)]}
    return net

# network
# (also defined by num_inputs & num_outputs)
num_layers = 10
lstm_unit = "lstmp"
network = build_network(num_layers, lstm_unit)

# trainer
batching = "random"
batch_size = 5000
max_seqs = 100
chunking = "50:25"
truncation = -1
num_epochs = 80
#pretrain = "default"
#pretrain_construction_algo = "from_input"
#debug_add_check_numerics_ops = True
#gradient_nan_inf_filter = True
gradient_clip = 0
optimizer_epsilon = 0.1
nadam = True
gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_file = "newbob.data"
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
model = "net-model/network"
cleanup_old_models = True

# log
log = "log/crnn.train.log"
log_verbosity = 5

