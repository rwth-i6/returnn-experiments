#!crnn/rnn.py
# kate: syntax python;
# based on ctc-fast-bw.fromscratch.sbt600.fprior.lexnorm.filterallos.nosilloop.tdp0.5l.pretrain.grad_noise03.nadam.lr05e_3.nbm6.nbrl.grad_clip_inf.config

import os
from subprocess import check_output
import numpy

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
num_inputs = 40  # Gammatone 40-dim
num_outputs = 9001 # 4501
EpochSplit = 6

_cf_cache = {}

def cf(filename):
	"""Cache manager"""
	if filename in _cf_cache:
		return _cf_cache[filename]
	if int(os.environ.get("CF_NOT_FOR_LOCAL", "1")) and check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
		print("use local file: %s" % filename)
		return filename  # for debugging
	cached_fn = check_output(["cf", filename]).strip()
	assert os.path.exists(cached_fn)
	_cf_cache[filename] = cached_fn
	return cached_fn

commonfiles = {
	"corpus": "/u/tuske/work/ASR/switchboard/corpus/xml/train.corpus.gz",
	"features": "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.bundle",
	"lexicon": "/u/tuske/work/ASR/switchboard/corpus/train.lex.v1_0_3.ci.gz",
	"alignment": "dependencies/tuske__2016_01_28__align.combined.train",
	"cart": "/u/tuske/work/ASR/switchboard/initalign/data/%s" % {9001: "cart-9000"}[num_outputs]
}

def get_sprint_dataset(data):
	assert data in ["train", "cv"]
	epochSplit = {"train": EpochSplit, "cv": 1}

	# see /u/tuske/work/ASR/switchboard/corpus/readme
	# and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
	files = {
		"config": "config/training.config",
		#"segments": "dependencies/seg_%s" % {"train":"train_align_compare10", "cv":"train_align_compare10"}[data]
		"segments": "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000"}[data]
	}
	files.update(commonfiles)
	for k, v in sorted(files.items()):
		assert os.path.exists(v), "%s %r does not exist" % (k, v)
	estimated_num_seqs = {"train": 227047, "cv": 3000}  # wc -l segment-file

	# features: /u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.*
	args = [
	"--config=" + files["config"],
	lambda: "--*.corpus.file=" + cf(files["corpus"]),
	lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
	"--*.corpus.segment-order-shuffle=true",
	"--*.segment-order-sort-by-time-length=true",
	"--*.segment-order-sort-by-time-length-chunk-size=%i" % {"train": (EpochSplit or 1) * 1000, "cv": -1}[data],
	"--*.state-tying.type=cart",
	lambda: "--*.state-tying.file=" + cf(files["cart"]),
	"--*.trainer-output-dimension=%i" % num_outputs,
	lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
	lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
	lambda: "--*.feature-cache-path=" + cf(files["features"]),
	#"--*.mean-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/mean",
	#"--*.variance-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/std",
	"--*.log-channel.file=log/crnn.sprint.dataset-%s.log" % data,
	"--*.window-size=1"
	]
	d = {
	"class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
	"sprintConfigStr": args,
	"partitionEpoch": epochSplit[data],
	"estimated_num_seqs": estimated_num_seqs[data] // (epochSplit[data] or 1)}
	d.update(sprint_interface_dataset_opts)
	return d

sprint_interface_dataset_opts = {"input_stddev": 3.}

def parse_tdp_config(s):
	s = s.replace(" ", "").replace("\t", "")
	return ["--*.tdp.%s" % l.strip() for l in s.splitlines() if l.strip()]

def get_sprint_error_signal_proc_args():
	files = commonfiles.copy()
	for k, v in sorted(files.items()):
		assert os.path.exists(v), "%s %r does not exist" % (k, v)
	return [
		"--config=config/ctc.train.config",
		"--action=python-control",
		"--python-control-loop-type=python-control-loop",
		"--*.python-segment-order=false",
		"--*.extract-features=false",  # we don't need features
		lambda: "--*.corpus.file=" + cf(files["corpus"]),
		"--*.state-tying.type=cart",
		lambda: "--*.state-tying.file=" + cf(files["cart"]),
		lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
		"--*.feature-cache-path=should-not-be-needed",
		"--*.alignment-cache-path=should-not-be-needed",
		"--*.prior-file=dependencies/prior-fixed-f32.xml",
		"--*.lexicon.normalize-pronunciation=true",
		"--*.transducer-builder-filter-out-invalid-allophones=true",
		"--*.fix-allophone-context-at-word-boundaries=true",
		"--*.allow-for-silence-repetitions=false",
		"--*.normalize-lemma-sequence-scores=true",
		"--*.number-of-classes=%i" % num_outputs
	] + parse_tdp_config("""
*.loop                  = %(loop)f
*.forward               = %(forward)f
*.skip                  = infinity
*.exit                  = %(forward)f
entry-m1.forward        = 0
entry-m2.forward        = 0
entry-m1.loop           = infinity
entry-m2.loop           = infinity
silence.loop            = %(sloop)f
silence.forward         = %(sforward)f
silence.skip            = infinity
silence.exit            = %(sforward)f
""" % {
#"loop": -numpy.log(0.65), "forward": -numpy.log(0.35),
#"sloop": -numpy.log(0.97), "sforward": -numpy.log(0.03)
#"sloop": -numpy.log(0.65), "sforward": -numpy.log(0.35)
#"loop": 0, "forward": 0, "sloop": 0, "sforward": 0
"loop": -numpy.log(0.5), "forward": -numpy.log(0.5),
"sloop": -numpy.log(0.5), "sforward": -numpy.log(0.5)
})

sprint_loss_opts = {
	"sprintExecPath": "sprint-executables/nn-trainer",
	"sprintConfigStr": "config:get_sprint_error_signal_proc_args",
	"minPythonControlVersion": 4,
	"sprintControlConfig": {},
	"numInstances": 2  # For debugging, better set this to 1.
}

train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
from Pretrain import WrapEpochValue
num_inputs = 40  # Gammatone 40-dim
num_outputs = 9001 # 4501
network = {
"lstm0_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1 },
"lstm0_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1 },

"lstm1_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm0_fw", "lstm0_bw"] },
"lstm1_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm0_fw", "lstm0_bw"] },

"lstm2_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm1_fw", "lstm1_bw"] },
"lstm2_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm1_fw", "lstm1_bw"] },

"lstm3_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm2_fw", "lstm2_bw"] },
"lstm3_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm2_fw", "lstm2_bw"] },

"lstm4_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm3_fw", "lstm3_bw"] },
"lstm4_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm3_fw", "lstm3_bw"] },

"lstm5_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm4_fw", "lstm4_bw"] },
"lstm5_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm4_fw", "lstm4_bw"] },

"lstm_out": {"class": "softmax", "n_out": num_outputs, "from" : ["lstm5_fw", "lstm5_bw"]},
"lstm_out_prior": {
    "class": "accumulate_mean", "exp_average": 0.001,
    "is_prob_distribution": True, "from": ["lstm_out"]},
# log-likelihood: combine out + prior
"lstm_out_scores": {
    "class": "combine", "kind": "eval", "from": ["lstm_out", "lstm_out_prior"],
    "eval": "safe_log(source(0)) * am_scale - safe_log(source(1)) * prior_scale",
    "eval_locals": {
        "am_scale": WrapEpochValue(lambda epoch: numpy.clip(0.05 * epoch, 0.1, 0.3)),
        "prior_scale": WrapEpochValue(lambda epoch: 0.5 * numpy.clip(0.05 * epoch, 0.1, 0.3))
        }
},
"lstm_out_bw": {"class": "fast_bw", "align_target": "sprint", "sprint_opts": sprint_loss_opts, "from": ["lstm_out_scores"]},
# CE smoothing with ff_out_bw
"lstm_out_bw_smooth": {
    "class": "combine", "kind": "eval", "from": ["lstm_out_bw", "ff_out_bw"],
    "eval": "source(0) * (1 - ce_smoothing) + source(1) * ce_smoothing",
    "eval_locals": {"ce_smoothing": WrapEpochValue(lambda epoch: max(0.5, 0.9 - 0.1 * epoch))},
    "eval_for_output_loss": True
},

"output" :   {
    "class" : "copy", "loss" : "via_layer", "from" : ["lstm_out"],
    "loss_opts": {
        "loss_wrt_to_act_in": "softmax",
        "align_layer": "lstm_out_bw_smooth"
    }},

"ff_in_window": {"class": "window", "window_size": 17, "trainable": False},
"ff_in": {"class": "merge_dims", "axes": "except_time", "from": ["ff_in_window"], "trainable": False},
"ff0": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff_in"]},
"ff1": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff0"]},
"ff2": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff1"]},
"ff3": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff2"]},
"ff4": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff3"]},
"ff5": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff4"]},
"ff6": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff5"]},
"ff7": {"class": "hidden", "activation": "relu", "n_out": 500, "L2": 0.01, "from": ["ff6"]},
"ff_out": {"class": "softmax", "n_out": num_outputs, "from" : ["ff7"]},
"ff_out_prior": {
    "class": "accumulate_mean", "exp_average": 0.001,
    "is_prob_distribution": True, "from": ["ff_out"]},
# combine out + prior
"ff_out_scores": {
    "class": "combine", "kind": "eval", "from": ["ff_out", "ff_out_prior"],
    "eval": "safe_log(source(0)) * am_scale - safe_log(source(1)) * prior_scale",
    "eval_locals": {
        "am_scale": WrapEpochValue(lambda epoch: numpy.clip(0.05 * epoch, 0.1, 0.3)),
        "prior_scale": WrapEpochValue(lambda epoch: 0.5 * numpy.clip(0.05 * epoch, 0.1, 0.3))
        }
},
"ff_out_bw": {"class": "fast_bw", "align_target": "sprint", "sprint_opts": sprint_loss_opts, "from": ["ff_out_scores"]},

"ff_output": {
    "class" : "copy", "loss" : "via_layer", "from" : ["ff_out"],
    "loss_opts": {
        "loss_wrt_to_act_in": "softmax",
        "align_layer": "ff_out_bw"
    }}
}

debug_print_layer_output_template = True
model = "net-model/network"
cleanup_old_models = True
pretrain = "default"
pretrain_construction_algo = "from_input"
pretrain_repetitions = {'default': 0, 'final': 6}

# trainer
log_batch_size = True
tf_log_memory_usage = True
batching = "random"
#batch_size = 4000
#max_seqs = 40
#max_seq_length = batch_size
batch_size = 10000
max_seqs = 200
max_seq_length = 4000
#chunking = ""  # no chunking
truncation = -1
num_epochs = 120
#debug_grad_summaries = True
#debug_save_updater_vars = True
#debug_add_check_numerics_ops = True
gradient_clip = 0
nadam = True
gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
learning_rate_file = "newbob.data"

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.seq-train.%s.log" % task
log_verbosity = 5

