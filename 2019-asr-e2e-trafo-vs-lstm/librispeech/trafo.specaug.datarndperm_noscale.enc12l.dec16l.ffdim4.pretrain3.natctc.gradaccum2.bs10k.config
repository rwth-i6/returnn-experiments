#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
import numpy
from subprocess import check_output, CalledProcessError
from Pretrain import WrapEpochValue

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# data
num_inputs = 40
num_outputs = {"classes": (10025, 1), "data": (num_inputs, 2)}  # see vocab
EpochSplit = 20


def get_dataset(key, subset=None, train_partition_epoch=None):
    d = {
        'class': 'LibriSpeechCorpus',
        'path': 'base/dataset/ogg-zips',
        "use_zip": True,
        "use_ogg": True,
        "use_cache_manager": True,
        "prefix": key,
        "bpe": {
            'bpe_file': 'base/dataset/trans.bpe.codes',
            'vocab_file': 'base/dataset/trans.bpe.vocab',
            'seq_postfix': [0],
            'unknown_label': '<unk>'},
        "audio": {
            "norm_mean": "base/dataset/stats.mean.txt",
            "norm_std_dev": "base/dataset/stats.std_dev.txt"},
    }
    if key.startswith("train"):
        d["partition_epoch"] = train_partition_epoch
        if key == "train":
            d["epoch_wise_filter"] = {
                (1, 5): {
                    'use_new_filter': True,
                    'max_mean_len': 50,  # chars
                    'subdirs': ['train-clean-100', 'train-clean-360']},
                (5, 10): {
                    'use_new_filter': True,
                    'max_mean_len': 150,  # chars
                    'subdirs': ['train-clean-100', 'train-clean-360']},
                (11, 20): {
                    'use_new_filter': True,
                    'subdirs': ['train-clean-100', 'train-clean-360']},
                }
        d["audio"]["random_permute"] = {
            "rnd_scale_lower": 1., "rnd_scale_upper": 1.,
            }
        num_seqs = 281241  # total
        d["seq_ordering"] = "laplace:%i" % (num_seqs // 1000)
    else:
        d["fixed_random_seed"] = 1
        d["seq_ordering"] = "sorted_reverse"
    if subset:
        d["fixed_random_subset"] = subset  # faster
    return d


train = get_dataset("train", train_partition_epoch=EpochSplit)
dev = get_dataset("dev", subset=3000)
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
target = "classes"

FinalNumLayers = 16
Dim = 512
TrafoDefaultKwargs = dict(
    encN=12, decN=FinalNumLayers,
    lstm_dim=Dim * 2, FFDim=Dim * 4, EncKeyTotalDim=Dim, EncValueTotalDim=Dim, AttNumHeads=8,
    dropout=0.1, label_smoothing=0.1)





def summary(name, x):
    """
    :param str name:
    :param tf.Tensor x: (batch,time,feature)
    """
    import tensorflow as tf
    # tf.summary.image wants [batch_size, height,  width, channels],
    # we have (batch, time, feature).
    img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
    img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
    tf.summary.image(name, img, max_outputs=10)
    tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
    mean = tf.reduce_mean(x)
    tf.summary.scalar("%s_mean" % name, mean)
    stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
    tf.summary.scalar("%s_stddev" % name, stddev)
    tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, axis, pos, max_amount):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int axis:
    :param tf.Tensor pos: (batch,)
    :param int max_amount: inclusive
    """
    import tensorflow as tf
    ndim = x.get_shape().ndims
    n_batch = tf.shape(x)[0]
    dim = tf.shape(x)[axis]
    amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
    pos2 = tf.minimum(pos + amount, dim)
    idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
    pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
    pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
    cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
    cond = tf.reshape(cond, [tf.shape(x)[i] if i in (0, axis) else 1 for i in range(ndim)])
    from TFUtil import where_bc
    x = where_bc(cond, 0.0, x)
    return x


def random_mask(x, axis, min_num, max_num, max_dims):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int axis:
    :param int|tf.Tensor min_num:
    :param int|tf.Tensor max_num: inclusive
    :param int max_dims: inclusive
    """
    import tensorflow as tf
    n_batch = tf.shape(x)[0]
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
    # https://github.com/tensorflow/tensorflow/issues/9260
    # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
    z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
    _, indices = tf.nn.top_k(z, tf.reduce_max(num))
    # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
    # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
    _, x = tf.while_loop(
        cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
        body=lambda i, x: (
            i + 1, 
            tf.where(
                tf.less(i, num),
                _mask(x, axis=axis, pos=indices[:, i], max_amount=max_dims),
                x)),
        loop_vars=(0, x))
    return x


def transform(x, network):
    import tensorflow as tf
    # summary("features", x)
    x = tf.clip_by_value(x, -3.0, 3.0)
    #summary("features_clip", x)
    def get_masked():
        x_masked = x
        x_masked = random_mask(x_masked, axis=1, min_num=1, max_num=tf.maximum(tf.shape(x)[1] // 100, 1), max_dims=20)
        x_masked = random_mask(x_masked, axis=2, min_num=1, max_num=2, max_dims=num_inputs // 5)
        #summary("features_mask", x_masked)
        return x_masked
    x = network.cond_on_train(get_masked, lambda: x)
    return x



class TransformerNetwork:

    def __init__(self, **kwargs):
        self._init(**kwargs)

    def _init(self, encN, decN,
                 lstm_dim, FFDim, EncKeyTotalDim, EncValueTotalDim, AttNumHeads,
                 dropout, label_smoothing):
        self.encN = encN
        self.decN = decN
        self.lstm_dim = lstm_dim
        self.FFDim = FFDim
        self.EncKeyTotalDim = EncKeyTotalDim
        self.AttNumHeads = AttNumHeads
        self.EncKeyPerHeadDim = self.EncKeyTotalDim // self.AttNumHeads
        self.EncValueTotalDim = EncValueTotalDim
        self.EncValuePerHeadDim = self.EncValueTotalDim // self.AttNumHeads
        self.embed_weight = self.EncValueTotalDim ** 0.5

        self.embed_dropout = 0.0
        self.lstm_dropout = dropout
        self.postprocess_dropout = dropout
        self.act_dropout = dropout
        self.attention_dropout = dropout
        self.label_smoothing = label_smoothing

        self.ff_init = "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=0.78)"

    def add_trafo_enc_layer(self, d, inp, output):
        d[output + '_self_att_laynorm'] = {"class": "layer_norm", "from": [inp]}
        d[output + '_self_att_att'] = {"class": "self_attention", "num_heads": self.AttNumHeads,
                                    "total_key_dim": self.EncKeyTotalDim,
                                    "n_out": self.EncValueTotalDim, "from": [output + '_self_att_laynorm'],
                                    "attention_left_only": False, "attention_dropout": self.attention_dropout, "forward_weights_init": self.ff_init}
        d[output + '_self_att_lin'] = {"class": "linear", "activation": None, "with_bias": False,
                                       "from": [output + '_self_att_att'], "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_self_att_drop'] = {"class": "dropout", "from": [output + '_self_att_lin'], "dropout": self.postprocess_dropout}
        d[output + '_self_att_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_self_att_drop'],
                                       "n_out": self.EncValueTotalDim}
        #####
        d[output + '_ff_laynorm'] = {"class": "layer_norm", "from": [output + '_self_att_out']}
        d[output + '_ff_conv1'] = {"class": "linear", "activation": "relu", "with_bias": True,
                                   "from": [output + '_ff_laynorm'],
                                   "n_out": self.FFDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_conv2'] = {"class": "linear", "activation": None, "with_bias": True,
                                   "from": [output + '_ff_conv1'], "dropout": self.act_dropout,
                                   "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_ff_conv2'], "dropout": self.postprocess_dropout}
        d[output + '_ff_out'] = {"class": "combine", "kind": "add",
                                 "from": [output + '_self_att_out', output + '_ff_drop'],
                                 "n_out": self.EncValueTotalDim}
        d[output] = {"class": "copy", "from": [output + '_ff_out']}

    def add_trafo_dec_layer(self, db, d, inp, output):
        pre_inp = [inp]
        d[output + '_self_att_laynorm'] = {"class": "layer_norm", "from": pre_inp}
        d[output + '_self_att_att'] = {"class": "self_attention", "num_heads": self.AttNumHeads,
                                    "total_key_dim": self.EncKeyTotalDim,
                                    "n_out": self.EncValueTotalDim, "from": [output + '_self_att_laynorm'],
                                    "attention_left_only": True, "attention_dropout": self.attention_dropout, "forward_weights_init": self.ff_init}
        d[output + '_self_att_lin'] = {"class": "linear", "activation": None, "with_bias": False,
                                       "from": [output + '_self_att_att'], "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_self_att_drop'] = {"class": "dropout", "from": [output + '_self_att_lin'], "dropout": self.postprocess_dropout}
        d[output + '_self_att_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_self_att_drop'],
                                       "n_out": self.EncValueTotalDim}
        #####
        d[output + '_att_laynorm'] = {"class": "layer_norm", "from": [output + '_self_att_out']}
        d[output + '_att_query0'] = {"class": "linear", "activation": None, "with_bias": False,
                                        "from": [output + '_att_laynorm'],
                                        "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_att_query'] = {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncKeyPerHeadDim),
                                    "from": [output + '_att_query0']}  # (B, H, D/H)
        db[output + '_att_key0'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                                    "n_out": self.EncKeyTotalDim, "forward_weights_init": self.ff_init}  # (B, enc-T, D)
        db[output + '_att_value0'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                                        "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        db[output + '_att_key'] = {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncKeyPerHeadDim),
                                    "from": [output + '_att_key0']}  # (B, enc-T, H, D/H)
        db[output + '_att_value'] = {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncValuePerHeadDim),
                                        "from": [output + '_att_value0']}  # (B, enc-T, H, D'/H)
        d[output + '_att_energy'] = {"class": "dot", "red1": -1, "red2": -1, "var1": "T", "var2": "T?",
                                        "from": ['base:' + output + '_att_key', output + '_att_query']}  # (B, H, enc-T, 1)
        d[output + '_att_weights'] = {"class": "softmax_over_spatial", "from": [output + '_att_energy'],
                                        "energy_factor": self.EncKeyPerHeadDim ** -0.5}  # (B, enc-T, H, 1)

        d[output + '_att_weights_drop'] = {"class": "dropout", "dropout_noise_shape": {"*": None},
                                            "from": [output + '_att_weights'], "dropout": self.attention_dropout}

        d[output + '_att0'] = {"class": "generic_attention", "weights": output + '_att_weights_drop',
                                "base": 'base:' + output + '_att_value'}  # (B, H, V)
        d[output + '_att_att'] = {"class": "merge_dims", "axes": "static",
                                "from": [output + '_att0']}  # (B, H*V) except_batch
        d[output + '_att_lin'] = {"class": "linear", "activation": None, "with_bias": False, "from": [output + '_att_att'],
                                    "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_att_drop'] = {"class": "dropout", "from": [output + '_att_lin'], "dropout": self.postprocess_dropout}
        d[output + '_att_out'] = {"class": "combine", "kind": "add",
                                    "from": [output + '_self_att_out', output + '_att_drop'],
                                    "n_out": self.EncValueTotalDim}
        #####
        d[output + '_ff_laynorm'] = {"class": "layer_norm", "from": [output + '_att_out']}
        d[output + '_ff_conv1'] = {"class": "linear", "activation": "relu", "with_bias": True,
                                   "from": [output + '_ff_laynorm'],
                                   "n_out": self.FFDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_conv2'] = {"class": "linear", "activation": None, "with_bias": True,
                                   "from": [output + '_ff_conv1'], "dropout": self.act_dropout,
                                   "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init}
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_ff_conv2'], "dropout": self.postprocess_dropout}
        d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [output + '_att_out', output + '_ff_drop'],
                                 "n_out": self.EncValueTotalDim}
        d[output] = {"class": "copy", "from": [output + '_ff_out']}

    def build(self):
        network = {
            "source": {"class": "eval", "eval": "self.network.get_config().typed_value('transform')(source(0), network=self.network)"},

            "lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : self.lstm_dim, "direction": 1, "from": ["source"] },
            "lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : self.lstm_dim, "direction": -1, "from": ["source"] },
            "lstm0_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (3,), "from": ["lstm0_fw", "lstm0_bw"], "trainable": False},

            "lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : self.lstm_dim, "direction": 1, "from": ["lstm0_pool"], "dropout": self.lstm_dropout },
            "lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : self.lstm_dim, "direction": -1, "from": ["lstm0_pool"], "dropout": self.lstm_dropout },
            "lstm1_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (2,), "from": ["lstm1_fw", "lstm1_bw"], "trainable": False},

            # trafo input here (excluding positional_encoding)
            "source_embed_raw": {"class": "linear", "from": "lstm1_pool", "n_out": self.EncValueTotalDim, "activation": None, "with_bias": False, "forward_weights_init": self.ff_init},
            "source_embed_weighted": {"class": "eval", "from": ["source_embed_raw"],
                                      "eval": "source(0) * %f" % self.embed_weight },
            "source_embed": {"class": "dropout", "from": ["source_embed_weighted"], "dropout": self.embed_dropout},

            #"source_embed_raw": {"class": "linear", "activation": None, "with_bias": False, "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init},
            #"source_embed_weighted": {"class": "eval", "from": ["source_embed_raw"],
            #                          "eval": "source(0) * %f" % self.embed_weight },
            #"source_embed_with_pos": {"class": "positional_encoding", "add_to_input": True,
            #                          "from": ["source_embed_weighted"]},
            #"source_embed": {"class": "dropout", "from": ["source_embed_with_pos"], "dropout": self.embed_dropout},

            # encoder stack is added by separate function
            "encoder": {"class": "layer_norm", "from": ["enc_%02d" % self.encN]},

            "output": {"class": "rec", "from": [], "unit": {
                'output': {'class': 'choice', 'target': target, 'beam_size': beam_size, 'from': ["output_prob"], "initial_output": 0}, # this is a vocab_id, make this flexible
                "end": {"class": "compare", "from": ["output"], "value": 0},
                'target_embed_raw': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['prev:output'],
                                     "n_out": self.EncValueTotalDim, "forward_weights_init": self.ff_init},
            # there seems to be no <s> in t2t, they seem to use just the zero vector
                "target_embed_weighted": {"class": "eval", "from": ["target_embed_raw"],
                                          "eval": "source(0) * %f" % self.embed_weight },
                #"target_embed_with_pos": {"class": "positional_encoding", "add_to_input": True,
                #                          "from": ["target_embed_weighted"]},
                #"target_embed": {"class": "dropout", "from": ["target_embed_with_pos"], "dropout": self.embed_dropout},
                "target_embed": {"class": "dropout", "from": ["target_embed_weighted"], "dropout": self.embed_dropout},

                # decoder stack is added by separate function
                "decoder": {"class": "layer_norm", "from": ["dec_%02d" % self.decN]},

                "output_prob": {
                    "class": "softmax", "from": ["decoder"], "dropout": 0.0,
                    "target": target,
                    "loss": "ce",
                    "loss_opts": {"label_smoothing": self.label_smoothing},
                    # "with_bias": False,
                    "forward_weights_init": self.ff_init
                }

            }, "target": target, "max_seq_len": "max_len_from('base:encoder')"},

            "decision": {
                "class": "decide", "from": ["output"], "loss": "edit_distance", "target": target,
                "loss_opts": {
                    # "debug_print": True
                }
            },

            "ctc": {"class": "softmax", "from": ["encoder"], "loss": "ctc", "target": target,
                "loss_opts": {"beam_width": 1, "use_native": True}}

        }

        self.add_trafo_enc_layer(network, "source_embed", "enc_01")
        for n in range(1, self.encN):
            self.add_trafo_enc_layer(network, "enc_%02d" % n, "enc_%02d" % (n+1))

        self.add_trafo_dec_layer(network, network["output"]["unit"], "target_embed", "dec_01")
        for n in range(1, self.decN):
            self.add_trafo_dec_layer(network, network["output"]["unit"], "dec_%02d" % n, "dec_%02d" % (n+1))


        return network

network = TransformerNetwork(**TrafoDefaultKwargs).build()

search_output_layer = "decision"
debug_print_layer_output_template = True

# trainer
batching = "random"
log_batch_size = True
batch_size = 10000
max_seqs = 200
max_seq_length = {"classes": 75}
#chunking = ""  # no chunking
truncation = -1

def custom_construction_algo(idx, net_dict):
    # For debugging, use: python3 ./crnn/Pretrain.py config... Maybe set repetitions=1 below.
    InitialDimFactor = 0.5
    StartNumLayers = 1

    dim_keys = {"lstm_dim", "FFDim", "EncKeyTotalDim", "EncValueTotalDim"}

    kwargs = TrafoDefaultKwargs.copy()
    kwargs["label_smoothing"] = 0  # Always disable. Enable only after pretraining.

    if idx <= 1:  # initially disable dropout
        kwargs["dropout"] = 0
    idx = max(idx - 1, 0)
    num_layers = 2 ** idx

    if num_layers > FinalNumLayers:
        return None

    kwargs["encN"] = min(num_layers, 12)
    kwargs["decN"] = num_layers
    
    grow_frac = 1.0 - float(FinalNumLayers - num_layers) / (FinalNumLayers - StartNumLayers)
    dim_frac = InitialDimFactor + (1.0 - InitialDimFactor) * grow_frac
    kwargs["dropout"] *= dim_frac
    for key in dim_keys:
        kwargs[key] = int(kwargs[key] * dim_frac / float(kwargs["AttNumHeads"])) * kwargs["AttNumHeads"]

    return TransformerNetwork(**kwargs).build()

pretrain = {"repetitions": 5, "copy_param_mode": "subset", "construction_algo": custom_construction_algo}

num_epochs = 250
model = "net-model/network"
cleanup_old_models = True
gradient_clip = 0
#gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
accum_grad_multiple_step = 2
#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
#debug_grad_summaries = True
gradient_noise = 0.0
learning_rate = 0.0008
learning_rates = list(numpy.linspace(0.0003, learning_rate, num=10))  # warmup
min_learning_rate = learning_rate / 50.
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = EpochSplit
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "newbob.data"

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5

