#!crnn/rnn.py
# LSTM baseline with 4 LSTM layers with dimension 2048.
# Projection layer only for input word embedding with dimension 128.

import os
from subprocess import check_output
from Util import cleanup_env_var_path

tf_session_opts = {'allow_soft_placement': True, 'log_device_placement': False}

# flag for sampled softmax
if config.has("use_full_softmax"):
   use_full_softmax = config.bool("use_full_softmax", True)
   print("** use_full_softmax %s" % use_full_softmax)
else:
   use_full_softmax = False

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

data_files = {
    "train": "/rwthfs/rz/cluster/home/ki626907/data/librispeech/train.lm.all.unk.txt.gz",
    "cv": "/rwthfs/rz/cluster/home/ki626907/data/librispeech/dev.clean.other.unk.txt.gz"}
vocab_file = "/rwthfs/rz/cluster/home/ki626907/data/librispeech/vocab.word.freq_sorted.200k.txt"

orth_replace_map_file = ""

num_inputs = 200002  # via vocab_file

train_epoch_split = 10

epoch_split = {"train": train_epoch_split, "cv": 1, "test": 1}
seq_order = {
    "train": "random",
    "cv": "sorted",
    "test": "default"}

def get_dataset(data):
    assert data in ["train", "cv", "test"]
    return {
        "class": "LmDataset",
        "corpus_file": data_files[data],
        "orth_symbols_map_file": vocab_file,
        "orth_replace_map_file": orth_replace_map_file,
        "word_based": True,
        "seq_end_symbol": "<sb>",
        "auto_replace_unknown_symbol": False,
        "unknown_symbol": "<UNK>",
        "add_delayed_seq_data": True,
        "delayed_seq_data_start_symbol": "<sb>",
        "seq_ordering": seq_order[data],
        "partition_epoch": epoch_split[data]
    }

train = get_dataset("train")
dev = get_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
num_outputs = {"data": {"dim": num_inputs, "sparse": True, "dtype": "int32"}}  # sparse data
num_outputs["delayed"] = num_outputs["data"]
target = "data"
network = {
"input": {"class": "linear", "n_out": 128, "activation": "identity",
          "forward_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "from": ["data:delayed"]},
"lstm0": {"class": "rec", "unit": "lstm",
          "forward_weights_init" : "random_normal_initializer(mean=0.0, stddev=0.1)",
          "recurrent_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "n_out": 2048, "dropout": 0.0, "L2": 0.0, "direction": 1, "from": ["input"]},
"lstm1": {"class": "rec", "unit": "lstm",
          "forward_weights_init" : "random_normal_initializer(mean=0.0, stddev=0.1)",
          "recurrent_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "n_out": 2048, "dropout": 0.0, "L2": 0.0, "direction": 1, "from": ["lstm0"]},
"lstm2": {"class": "rec", "unit": "lstm",
          "forward_weights_init" : "random_normal_initializer(mean=0.0, stddev=0.1)",
          "recurrent_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "n_out": 2048, "dropout": 0.0, "L2": 0.0, "direction": 1, "from": ["lstm1"]},
"lstm3": {"class": "rec", "unit": "lstm",
          "forward_weights_init" : "random_normal_initializer(mean=0.0, stddev=0.1)",
          "recurrent_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
          "n_out": 2048, "dropout": 0.0, "L2": 0.0, "direction": 1, "from": ["lstm2"]},
"output": {"class": "softmax", "dropout": 0.0, "use_transposed_weights": True,
           "loss_opts": {'num_sampled': 16384, 'use_full_softmax': use_full_softmax, 'nce_loss': False},
           "forward_weights_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
           "bias_init": "random_normal_initializer(mean=0.0, stddev=0.1)",
           "loss": "sampling_loss", "target": "data", "from": ["lstm3"]}
}

batching = "random"
batch_size = 900
max_seq_length = 602
max_seqs = 32
chunking = "0"
num_epochs = 50
gradient_clip_global_norm = 2.
gradient_noise = 0.
learning_rate = 1.
learning_rate_control = "newbob_rel"
learning_rate_control_relative_error_relative_lr = False
newbob_multi_num_epochs = train_epoch_split
newbob_relative_error_div_by_old = True

newbob_learning_rate_decay = 0.8
newbob_relative_error_threshold = -0.01
newbob_multi_update_interval = 1
learning_rate_control_error_measure = "dev_score_output:exp"


learning_rate_file = "newbob.data"
model = "net-model/network"

calculate_exp_loss = True
cleanup_old_models = True

# log
log = "log/crnn.%s.log" % task
log_verbosity = 4

