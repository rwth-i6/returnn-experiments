#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

# based on: /u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/Train.JmZDphtmhGAU/work/config.py
#{'additional_step_rule': 'Adam(learning_rate=0.001)',
 #'batch_size': 50,
 #'beam_size': 12,
 #'bleu_script': None,
 #'bos_token': '<S>',
 #'dec_embed': 620,
 #'dec_nhids': 1000,
 #'decoder_transition': 'LSTM',
 #'dropout': 0.3,
 #'enc_embed': 620,
 #'enc_layers': [{'dim': 620, 'type': 'LookUp'},
                #{'dim': 1000, 'type': 'BidirectionalLSTM'},
                #{'dim': 1000, 'type': 'BidirectionalLSTM'}],
 #'enc_nhids': 1000,
 #'eos_token': '</S>',
 #'fertility': 2,
 #'finish_after': 300000,
 #'hook_samples': 2,
 #'level': 'word',
 #'load_from': '/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/Train.JmZDphtmhGAU/output/model',
 #'max_seq_len': 75,
 #'normalized_bleu': True,
 #'output_val_set': True,
 #'reload': True,
 #'sampling_freq': 1000,
 #'save_freq': 5000,
 #'save_keep_freq': 10000,
 #'save_to': '/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/Train.JmZDphtmhGAU/output/model',
 #'sort_k_batches': 20,
 #'src_data': '/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/ShuffleParallel.Qe43qekYTgDo/output/src.shuf.gz',
 #'src_vocab': '/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/Vocabulary.xKO3PFhqSy3s/output/vocab.pkl',
 #'src_vocab_size': 24236,
 #'step_clipping': 1.0,
 #'step_rule': 'Scale',
 #'stream': 'stream',
 #'subword_units': 20000,
 #'trg_data': '/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/ShuffleParallel.Qe43qekYTgDo/output/trg.shuf.gz',
 #'trg_vocab': '/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/Vocabulary.p2LiYSQdvp95/output/vocab.pkl',
 #'trg_vocab_size': 24214,
 #'unk_id': 1,
 #'unk_token': '<UNK>',
 #'use_joint_subwords': True,
 #'use_sum_alignment_feedback': True,
 #'weight_noise_ff': False,
 #'weight_noise_rec': False,
 #'weight_scale': 0.01}

# run:
# returnn/rnn.py returnn.config

import os
from subprocess import check_output
import numpy

my_dir = os.path.dirname(os.path.abspath(__file__))

debug_mode = False
if int(os.environ.get("DEBUG", "0")) or int(os.environ.get("TFDEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train'}"
num_outputs = {'data': [24236, 1], 'classes': [24214, 1]}
num_inputs = num_outputs["data"][0]

# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train', 'seq_ordering':'sorted'}" --get_num_seqs
num_seqs = {'train': 4218414, 'dev': 3003}
# Via JTP: 5 times seeing the whole train dataset is enough.
EpochSplit = 50
SeqOrderTrainBins = num_seqs["train"] // 1000
TrainSeqOrder = "laplace:%i" % SeqOrderTrainBins
if debug_mode:
    TrainSeqOrder = "default"

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if int(os.environ.get("CF_NOT_FOR_LOCAL", "1")) and check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip()
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


def get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    return {
        "class": "TranslationDataset",
        "path": "base/dataset",
        "file_postfix": data,
        "source_postfix": " </S>",
        "target_postfix": " </S>",
        "partition_epoch": epochSplit,
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default"),
        "estimated_num_seqs": (num_seqs.get(data, None) // epochSplit) if data in num_seqs else None}

train = get_dataset("train")
dev = get_dataset("dev")
dev_short = get_dataset("dev_short")
test_data = get_dataset("test")
eval_data = get_dataset("eval")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
network = {
"source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": 620},

"lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 500, "direction": 1, "from": ["source_embed"] },
"lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 500, "direction": -1, "from": ["source_embed"] },

"lstm1" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm0_fw", "lstm0_bw"] },
"lstm2" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm1"] },
"lstm3" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm2"] },
"lstm4" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm3"] },
"lstm5" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm4"] },

"encoder": {"class": "copy", "from": ["lstm5"]},
"enc_ctx": {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"], "n_out": 1000},

"output": {"class": "rec", "from": [], "unit": {
    'output': {'class': 'choice', 'target': 'classes', 'beam_size': beam_size, 'from': ["output_prob"], "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 620, "initial_output": 0},  # feedback_input
    "prev_target_embed_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:target_embed"], "n_out": 1000},
    "target_rnn": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:target_embed", "prev:readout"], "n_out": 1000},
    "target_rnn_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["target_rnn"], "n_out": 1000},
    
    "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": 1000},
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "prev_target_embed_transformed", "target_rnn_transformed"], "n_out": 1000},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},  # (B, enc-T, 1)
    "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},  # (B, enc-T, 1)
    "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights"],
        "eval": "source(0) + source(1)", "out_type": {"dim": 1, "shape": (None, 1)}},
    "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},
    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_rnn", "att"], "n_out": 1000},
    "readout": {"class": "linear", "from": ["s", "att"], "activation": "tanh", "n_out": 1000, "dropout": 0.3},
    "output_prob": {"class": "softmax", "from": ["readout"], "target": "classes", "loss": "ce"}
}, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3"},

"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        #"debug_print": True
        }
    }
}
search_output_layer = "decision"
debug_print_layer_output_template = True

# trainer
batching = "random"
log_batch_size = True
tf_log_memory_usage = True
batch_size = 7500
max_seqs = 200
max_seq_length = 60
#chunking = ""  # no chunking
truncation = -1
num_epochs = 200
model = "net-model/network"
cleanup_old_models = True
pretrain = {"output_layers": ["encoder"], "input_layers": ["source_embed"], "repetitions": 5}
gradient_clip = 0
#gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
gradient_noise = 0.0
learning_rate = 0.001
learning_rates = list(numpy.linspace(0.0001, 0.001, num=5))  # warmup
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "newbob.data"

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5



#blocks_mt_model = "/u/peter/experiments/wmt/2017/2017-08-14_de-en/work/blocks/Train.JmZDphtmhGAU/output/model.0300000"
#blocks_mt_model = "%s/blocks-params.npz" % my_dir
#blocks_debug_dump_output = "/Users/az/Programmierung/blocks-mt-model-import/dump-seq0"
