#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

# via:
# /u/irie/setups/switchboard/2018-02-13--end2end-zeyer/config-train/bpe_1k.multihead-mlp-h1.red8.enc6l.encdrop03.decbs.ls01.pretrain2.nbd07.config
# Kazuki BPE1k baseline, from Interspeech paper.

import os, sys
import numpy
from subprocess import check_output, CalledProcessError
from returnn.tf.util.basic import DimensionTag

config = globals()["config"]  # make PyCharm happy

# Workaround for openblas hanging:
# * https://github.com/tensorflow/tensorflow/issues/13802
# * https://github.com/rwth-i6/returnn/issues/323#issuecomment-725384762
patch_atfork = True

# task
use_tensorflow = True
task = config.value("task", "train")
# device = "gpu"
multiprocessing = True
update_on_device = True

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
  # print("** DEBUG MODE")
  debug_mode = True

if config.has("out_beam_size"):
  out_beam_size = config.int("out_beam_size", 0)
  print("** out_beam_size %i" % out_beam_size, file=sys.stderr)
else:
  out_beam_size = 12

if config.has("t_beam_size"):
  t_beam_size = config.int("t_beam_size", 0)
  print("** t_beam_size %i" % t_beam_size, file=sys.stderr)
else:
  t_beam_size = 100

_cf_cache = {}


def cf(filename):
  """Cache manager"""
  if filename in _cf_cache:
    return _cf_cache[filename]
  if debug_mode or check_output(["hostname"]).strip().decode("utf8") in ["cluster-cn-211", "sulfid"]:
    print("use local file: %s" % filename)
    return filename  # for debugging
  try:
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
  except CalledProcessError:
    print("Cache manager: Error occured, using local file")
    return filename
  assert os.path.exists(cached_fn)
  _cf_cache[filename] = cached_fn
  return cached_fn


# data
target = "bpe"
target_num_labels = 1030
targetb_num_labels = target_num_labels + 1  # with blank
targetb_blank_idx = target_num_labels
time_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="time")
output_len_tag = DimensionTag(kind=DimensionTag.Types.Spatial, description="output-len")  # it's downsampled time
# use "same_dim_tags_as": {"t": time_tag} if same time tag ("data" and "alignment"). e.g. for RNA. not for RNN-T.
extern_data = {
  "data": {"dim": 40, "same_dim_tags_as": {"t": time_tag}},  # Gammatone 40-dim
  "alignment": {"dim": targetb_num_labels, "sparse": True, "same_dim_tags_as": {"t": output_len_tag}},
  # for cheating experiment:
  # "target_t": {"dim": None, "sparse": True, "available_for_inference": True},
  "relative_t": {"dim": None, "sparse": True, "available_for_inference": False},
}
if task != "train":
  # During train, we add this via the network (from prev alignment, or linear seg). Otherwise it's not available.
  extern_data["targetb"] = {"dim": targetb_num_labels, "sparse": True, "available_for_inference": False}
  extern_data[target] = {"dim": target_num_labels, "sparse": True}  # must not be used for chunked training
EpochSplit = 6


_import_baseline_setup = "ctcalign.prior0.lstm6l.withchar.lrkeyfix"
_alignment = "%s.epoch-150" % _import_baseline_setup

def get_sprint_dataset(data):
  assert data in {"train", "devtrain", "cv", "dev", "hub5e_01", "rt03s"}
  epoch_split = {"train": EpochSplit}.get(data, 1)
  corpus_name = {"cv": "train", "devtrain": "train"}.get(data, data)  # train, dev, hub5e_01, rt03s
  hdf_files = None
  if data in {"train", "cv", "devtrain"}:
    hdf_files = ["base/dump-align/data/%s.data-%s.hdf" % (_alignment, {"cv": "dev", "devtrain": "train"}.get(data, data))]

  # see /u/tuske/work/ASR/switchboard/corpus/readme
  # and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
  files = {}
  files["config"] = "config/training.config"
  files["corpus"] = "/work/asr3/irie/data/switchboard/corpora/%s.corpus.gz" % corpus_name
  if data in {"train", "cv", "devtrain"}:
    files["segments"] = "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000", "devtrain": "train_head3000"}[data]
  files["features"] = "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.%s.bundle" % corpus_name
  for k, v in sorted(files.items()):
    assert os.path.exists(v), "%s %r does not exist" % (k, v)
  estimated_num_seqs = {"train": 227047, "cv": 3000, "devtrain": 3000}  # wc -l segment-file

  args = [
    "--config=" + files["config"],
    lambda: "--*.corpus.file=" + cf(files["corpus"]),
    lambda: "--*.corpus.segments.file=" + (cf(files["segments"]) if "segments" in files else ""),
    lambda: "--*.feature-cache-path=" + cf(files["features"]),
    "--*.log-channel.file=/dev/null",
    "--*.window-size=1",
    ]
  if not hdf_files:
    args += [
      "--*.corpus.segment-order-shuffle=true",
      "--*.segment-order-sort-by-time-length=true",
      "--*.segment-order-sort-by-time-length-chunk-size=%i" % {"train": epoch_split * 1000}.get(data, -1),
      ]
  d = {
    "class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
    "sprintConfigStr": args,
    "suppress_load_seqs_print": True,  # less verbose
  }
  d.update(sprint_interface_dataset_opts)
  partition_epochs_opts = {
    "partition_epoch": epoch_split,
    "estimated_num_seqs": (estimated_num_seqs[data] // epoch_split) if data in estimated_num_seqs else None,
  }
  if hdf_files:
    align_opts = {
      "class": "HDFDataset", "files": hdf_files,
      "use_cache_manager": True,
      "seq_list_filter_file": files["segments"],  # otherwise not right selection
      #"unique_seq_tags": True  # dev set can exist multiple times
    }
    align_opts.update(partition_epochs_opts)  # this dataset will control the seq list
    if data == "train":
      align_opts["seq_ordering"] = "laplace:%i" % (estimated_num_seqs[data] // 1000)
      align_opts["seq_order_seq_lens_file"] = "/u/zeyer/setups/switchboard/dataset/data/seq-lens.train.txt.gz"
    d = {
      "class": "MetaDataset",
      "datasets": {"sprint": d, "align": align_opts},
      "data_map": {
        "data": ("sprint", "data"),
        # target: ("sprint", target),
        "alignment": ("align", "data"),
        #"align_score": ("align", "scores")
      },
      "seq_order_control_dataset": "align",  # it must support get_all_tags
    }
  else:
    d.update(partition_epochs_opts)
  return d


sprint_interface_dataset_opts = {
  "input_stddev": 3.,
  "bpe": {
    'bpe_file': '/work/asr3/irie/data/switchboard/subword_clean/ready/swbd_clean.bpe_code_1k',
    'vocab_file': '/work/asr3/irie/data/switchboard/subword_clean/ready/vocab.swbd_clean.bpe_code_1k',
    # 'seq_postfix': [0]  # no EOS needed for RNN-T
  }}

# train = get_sprint_dataset("train")
# dev = get_sprint_dataset("cv")
# eval_datasets = {"devtrain": get_sprint_dataset("devtrain")}


cache_size = "0"
window = 1


# Note: We control the warmup in the pretrain construction.
learning_rate = 0.001
min_learning_rate = learning_rate / 50.


def summary(name, x):
  """
  :param str name:
  :param tf.Tensor x: (batch,time,feature)
  """
  import tensorflow as tf
  # tf.summary.image wants [batch_size, height,  width, channels],
  # we have (batch, time, feature).
  img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
  img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
  tf.summary.image(name, img, max_outputs=10)
  tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
  mean = tf.reduce_mean(x)
  tf.summary.scalar("%s_mean" % name, mean)
  stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
  tf.summary.scalar("%s_stddev" % name, stddev)
  tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, batch_axis, axis, pos, max_amount, mask_value=0.):
  """
  :param tf.Tensor x: (batch,time,[feature])
  :param int batch_axis:
  :param int axis:
  :param tf.Tensor pos: (batch,)
  :param int|tf.Tensor max_amount: inclusive
  :param float|int mask_value:
  """
  import tensorflow as tf
  ndim = x.get_shape().ndims
  n_batch = tf.shape(x)[batch_axis]
  dim = tf.shape(x)[axis]
  amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
  pos2 = tf.minimum(pos + amount, dim)
  idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
  pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
  pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
  cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
  if batch_axis > axis:
    cond = tf.transpose(cond)  # (dim,batch)
  cond = tf.reshape(cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)])
  from TFUtil import where_bc
  x = where_bc(cond, mask_value, x)
  return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims, mask_value=0.):
  """
  :param tf.Tensor x: (batch,time,feature)
  :param int batch_axis:
  :param int axis:
  :param int|tf.Tensor min_num:
  :param int|tf.Tensor max_num: inclusive
  :param int|tf.Tensor max_dims: inclusive
  :param float|int mask_value:
  """
  import tensorflow as tf
  n_batch = tf.shape(x)[batch_axis]
  if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
    num = min_num
  else:
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
  # https://github.com/tensorflow/tensorflow/issues/9260
  # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
  z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
  _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
  # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
  # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
  if isinstance(num, int):
    for i in range(num):
      x = _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims, mask_value=mask_value)
  else:
    _, x = tf.while_loop(
      cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
      body=lambda i, x: (
        i + 1,
        tf.where(
          tf.less(i, num),
          _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims, mask_value=mask_value),
          x)),
      loop_vars=(0, x))
  return x


def transform(data, network, time_factor=1):
  x = data.placeholder
  import tensorflow as tf
  # summary("features", x)
  step = network.global_train_step
  step1 = tf.where(tf.greater_equal(step, 1000), 1, 0)
  step2 = tf.where(tf.greater_equal(step, 2000), 1, 0)
  def get_masked():
    x_masked = x
    x_masked = random_mask(
      x_masked, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
      min_num=step1 + step2, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis] // 100, 2) * (1 + step1 + step2 * 2),
      max_dims=20 // time_factor)
    x_masked = random_mask(
      x_masked, batch_axis=data.batch_dim_axis, axis=data.feature_dim_axis,
      min_num=step1 + step2, max_num=2 + step1 + step2 * 2,
      max_dims=data.dim // 5)
    #summary("features_mask", x_masked)
    return x_masked
  x = network.cond_on_train(get_masked, lambda: x)
  return x


def switchout_target(self, source, **kwargs):
  import tensorflow as tf
  from TFUtil import where_bc
  network = self.network
  time_factor = 6
  data = source(0, as_data=True)
  assert data.is_batch_major  # just not implemented otherwise
  x = data.placeholder
  def get_switched():
    x_ = x
    shape = tf.shape(x)
    n_batch = tf.shape(x)[data.batch_dim_axis]
    n_time = tf.shape(x)[data.time_dim_axis]
    take_rnd_mask = tf.less(tf.random_uniform(shape=shape, minval=0., maxval=1.), 0.05)
    take_blank_mask = tf.less(tf.random_uniform(shape=shape, minval=0., maxval=1.), 0.5)
    rnd_label = tf.random_uniform(shape=shape, minval=0, maxval=target_num_labels, dtype=tf.int32)
    rnd_label = where_bc(take_blank_mask, targetb_blank_idx, rnd_label)
    x_ = where_bc(take_rnd_mask, rnd_label, x_)
    x_ = random_mask(
      x_, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
      min_num=0, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis] // (50 // time_factor), 1),
      max_dims=20 // time_factor,
      mask_value=targetb_blank_idx)
    #x_ = tf.Print(x_, ["switch", x[0], "to", x_[0]], summarize=100)
    return x_
  x = network.cond_on_train(get_switched, lambda: x)
  return x


def targetb_linear(source, **kwargs):
  from TFUtil import get_rnnt_linear_aligned_output
  enc = source(1, as_data=True, auto_convert=False)
  dec = source(0, as_data=True, auto_convert=False)
  enc_lens = enc.get_sequence_lengths()
  dec_lens = dec.get_sequence_lengths()
  out, out_lens = get_rnnt_linear_aligned_output(
    input_lens=enc_lens,
    target_lens=dec_lens, targets=dec.get_placeholder_as_batch_major(),
    blank_label_idx=targetb_blank_idx,
    targets_consume_time=True)
  return out


def targetb_linear_out(sources, **kwargs):
  from TFUtil import Data
  enc = sources[1].output
  dec = sources[0].output
  size = enc.get_sequence_lengths() #  + dec.get_sequence_lengths()
  #output_len_tag.set_tag_on_size_tensor(size)
  return Data(name="targetb_linear", sparse=True, dim=targetb_num_labels, size_placeholder={0: size})


def targetb_search_or_fallback(source, **kwargs):
  import tensorflow as tf
  from TFUtil import where_bc
  ts_linear = source(0)  # (B,T)
  ts_search = source(1)  # (B,T)
  l = source(2, auto_convert=False)  # (B,)
  return where_bc(tf.less(l[:, None], 0.01), ts_search, ts_linear)


def targetb_recomb_train(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):
  """
  :param ChoiceLayer layer:
  :param tf.Tensor batch_dim: scalar
  :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
  :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
  :param tf.Tensor end_flags: (batch,base_beam_in)
  :param tf.Tensor base_beam_in: int32 scalar, 1 or prev beam size
  :rtype: tf.Tensor
  :return: (batch,base_beam_in,dim), combined scores
  """
  import tensorflow as tf
  from TFUtil import where_bc, nd_indices, tile_transposed
  scores = scores_in + scores_base  # (batch,beam,dim)
  dim = layer.output.dim

  u = layer.explicit_search_sources[0].output  # prev:u actually. [B*beam], pos in target [0..decT-1]
  assert u.shape == ()
  u_t = tf.reshape(tf.reshape(u.placeholder, (batch_dim, -1))[:,:base_beam_in], (-1,))  # u beam might differ from base_beam_in
  targets = layer.network.parent_net.extern_data.data[target]  # BPE targets, [B,decT]
  assert targets.shape == (None,) and targets.is_batch_major
  target_lens = targets.get_sequence_lengths()  # [B]
  target_lens_exp = tile_transposed(target_lens, axis=0, multiples=base_beam_in)  # [B*beam]
  missing_targets = target_lens_exp - u_t  # [B*beam]
  allow_target = tf.greater(missing_targets, 0)  # [B*beam]
  targets_exp = tile_transposed(targets.placeholder, axis=0, multiples=base_beam_in)  # [B*beam,decT]
  targets_u = tf.gather_nd(targets_exp, indices=nd_indices(where_bc(allow_target, u_t, 0)))  # [B*beam]
  targets_u = tf.reshape(targets_u, (batch_dim, base_beam_in))  # (batch,beam)
  allow_target = tf.reshape(allow_target, (batch_dim, base_beam_in))  # (batch,beam)

  #t = layer.explicit_search_sources[1].output  # prev:t actually. [B*beam], pos in encoder [0..encT-1]
  #assert t.shape == ()
  #t_t = tf.reshape(tf.reshape(t.placeholder, (batch_dim, -1))[:,:base_beam_in], (-1,))  # t beam might differ from base_beam_in
  t_t = layer.network.get_rec_step_index() - 1  # scalar
  inputs = layer.network.parent_net.get_layer("encoder").output  # encoder, [B,encT]
  input_lens = inputs.get_sequence_lengths()  # [B]
  input_lens_exp = tile_transposed(input_lens, axis=0, multiples=base_beam_in)  # [B*beam]
  allow_blank = tf.less(missing_targets, input_lens_exp - t_t)  # [B*beam]
  allow_blank = tf.reshape(allow_blank, (batch_dim, base_beam_in))  # (batch,beam)

  dim_idxs = tf.range(dim)[None,None,:]  # (1,1,dim)
  masked_scores = where_bc(
    tf.logical_or(
      tf.logical_and(tf.equal(dim_idxs, targetb_blank_idx), allow_blank[:,:,None]),
      tf.logical_and(tf.equal(dim_idxs, targets_u[:,:,None]), allow_target[:,:,None])),
    scores, float("-inf"))

  return where_bc(end_flags[:,:,None], scores, masked_scores)


def get_vocab_tf():
  from GeneratingDataset import Vocabulary
  import TFUtil
  import tensorflow as tf
  vocab = Vocabulary.create_vocab(**sprint_interface_dataset_opts["bpe"])
  labels = vocab.labels  # bpe labels ("@@" at end, or not), excluding blank
  labels = [(l + " ").replace("@@ ", "") for l in labels] + [""]
  labels_t = TFUtil.get_shared_vocab(labels)
  return labels_t


def get_vocab_sym(i):
  """
  :param tf.Tensor i: e.g. [B], int32
  :return: same shape as input, string
  :rtype: tf.Tensor
  """
  import tensorflow as tf
  return tf.gather(params=get_vocab_tf(), indices=i)


def out_str(source, **kwargs):
  # ["prev:out_str", "output"]
  import tensorflow as tf
  from TFUtil import where_bc
  return source(0) + get_vocab_sym(source(1))


def get_filtered_score_op(verbose=False):
  cpp_code = """
    #include "tensorflow/core/framework/op.h"
    #include "tensorflow/core/framework/op_kernel.h"
    #include "tensorflow/core/framework/shape_inference.h"
    #include "tensorflow/core/framework/resource_mgr.h"
    #include "tensorflow/core/framework/resource_op_kernel.h"
    #include "tensorflow/core/framework/tensor.h"
    #include "tensorflow/core/platform/macros.h"
    #include "tensorflow/core/platform/mutex.h"
    #include "tensorflow/core/platform/types.h"
    #include "tensorflow/core/public/version.h"
    #include <cmath>
    #include <map>
    #include <set>
    #include <string>
    #include <tuple>

    using namespace tensorflow;

    REGISTER_OP("GetFilteredScore")
    .Input("prev_str: string")
    .Input("scores: float32")
    .Input("labels: string")
    .Output("new_scores: float32")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        c->set_output(0, c->input(1));
        return Status::OK();
    });

    class GetFilteredScoreOp : public OpKernel {
    public:
    using OpKernel::OpKernel;
    void Compute(OpKernelContext* context) override {
        const Tensor* prev_str = &context->input(0);
        const Tensor* scores = &context->input(1);
        const Tensor* labels = &context->input(2);

        int n_batch = prev_str->shape().dim_size(0);
        int n_beam = prev_str->shape().dim_size(1);

        Tensor* ret;
        OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({n_batch, n_beam}), &ret));
        for(int bat = 0; bat < n_batch; ++bat)
            for(int hyp = 0; hyp < n_beam; ++hyp)
                ret->tensor<float, 2>()(bat, hyp) = scores->tensor<float, 2>()(bat, hyp);
        
        for(int bat = 0; bat < n_batch; ++bat) {
            std::map<tstring, std::set<int> > new_hyps;  // seq -> set of hyp idx

            for(int hyp = 0; hyp < n_beam; ++hyp) {
                auto& seq_set = new_hyps[prev_str->tensor<tstring, 2>()(bat, hyp)];
                seq_set.insert(hyp);
            }

            for(const auto& items : new_hyps) {
                if(std::get<1>(items).size() > 1) {
                    float best_score = 0.;
                    int best_idx = -1;
                    for(int idx : std::get<1>(items)) {
                        float score = scores->tensor<float, 2>()(bat, idx);
                        if(score > best_score || best_idx == -1) {
                            best_score = score;
                            best_idx = idx;
                        }
                    }

                    float sum_score = 0.;
                    for(int idx : std::get<1>(items)) {
                        float score = scores->tensor<float, 2>()(bat, idx);
                        sum_score += expf(score - best_score);
                    }
                    sum_score = logf(sum_score) + best_score;

                    for(int idx : std::get<1>(items)) {
                        if(idx != best_idx)
                            ret->tensor<float, 2>()(bat, idx) = -std::numeric_limits<float>::infinity();
                        else
                            ret->tensor<float, 2>()(bat, idx) = sum_score;
                    }
                }
            }
        }
    }
    };
    REGISTER_KERNEL_BUILDER(Name("GetFilteredScore").Device(DEVICE_CPU), GetFilteredScoreOp);
    """
  from TFUtil import OpCodeCompiler
  compiler = OpCodeCompiler(
    base_name="GetFilteredScore", code_version=1, code=cpp_code,
    is_cpp=True, use_cuda_if_available=False, verbose=verbose)
  tf_mod = compiler.load_tf_module()
  return tf_mod.get_filtered_score


def get_filtered_score_cpp(prev_str, scores, labels):
  """
  :param tf.Tensor prev_str: (batch,beam)
  :param tf.Tensor scores: (batch,beam)
  :param list[bytes] labels: len (dim)
  :return: scores with logsumexp at best, others -inf, (batch,beam)
  :rtype: tf.Tensor
  """
  import TFUtil
  labels_t = TFUtil.get_shared_vocab(labels)
  import tensorflow as tf
  with tf.device("/cpu:0"):
    return get_filtered_score_op()(prev_str, scores, labels_t)


def targetb_recomb_recog(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):
  """
  :param ChoiceLayer layer:
  :param tf.Tensor batch_dim: scalar
  :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
  :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
  :param tf.Tensor end_flags: (batch,base_beam_in)
  :param tf.Tensor base_beam_in: int32 scalar, 1 or prev beam size
  :rtype: tf.Tensor
  :return: (batch,base_beam_in,dim), combined scores
  """
  import tensorflow as tf
  from TFUtil import where_bc, nd_indices, tile_transposed

  dim = layer.output.dim

  prev_str = layer.explicit_search_sources[0].output  # [B*beam], str
  prev_str_t = tf.reshape(prev_str.placeholder, (batch_dim, -1))[:,:base_beam_in]
  prev_out = layer.explicit_search_sources[1].output  # [B*beam], int32
  prev_out_t = tf.reshape(prev_out.placeholder, (batch_dim, -1))[:,:base_beam_in]

  from GeneratingDataset import Vocabulary
  import TFUtil
  import tensorflow as tf
  vocab = Vocabulary.create_vocab(**sprint_interface_dataset_opts["bpe"])
  labels = vocab.labels  # bpe labels ("@@" at end, or not), excluding blank
  labels = [(l + " ").replace("@@ ", "").encode("utf8") for l in labels] + [b""]

  # Pre-filter approx (should be much faster), sum approx (better).
  scores_base = tf.reshape(get_filtered_score_cpp(prev_str_t, tf.reshape(scores_base, (batch_dim, base_beam_in)), labels), (batch_dim, base_beam_in, 1))

  scores = scores_in + scores_base  # (batch,beam,dim)

  # Mask -> max approx, in all possible options, slow.
  #mask = get_score_mask_cpp(prev_str_t, prev_out_t, scores, labels)
  #masked_scores = where_bc(mask, scores, float("-inf"))
  # Sum approx in all possible options, slow.
  #masked_scores = get_new_score_cpp(prev_str_t, prev_out_t, scores, labels)

  #scores = where_bc(end_flags[:,:,None], scores, masked_scores)

  return scores


def t_recomb_recog(layer, batch_dim, scores_in, scores_base, base_beam_in, end_flags, **kwargs):
    """
    :param ChoiceLayer layer:
    :param tf.Tensor batch_dim: scalar
    :param tf.Tensor scores_base: (batch,base_beam_in,1). existing beam scores
    :param tf.Tensor scores_in: (batch,base_beam_in,dim). log prob frame distribution
    :param tf.Tensor end_flags: (batch,base_beam_in)
    :param int base_beam_in:
    :rtype: tf.Tensor
    :return: (batch,base_beam_in,dim), combined scores
    """
    import tensorflow as tf
    from TFUtil import where_bc
    end_flags = tf.expand_dims(end_flags, axis=-1)  # (batch,beam,1)
    scores = scores_in + scores_base  # (batch,beam,dim)
    best_idxs = tf.cast(tf.argmax(scores, axis=1), tf.int32)  # (batch,dim) -> beam idx
    mask = tf.equal(tf.range(base_beam_in)[None,:,None], best_idxs[:,None,:])  # (batch,beam,dim)
    mask2 = tf.equal(tf.range(base_beam_in)[None,:,None], base_beam_in - 1)  # keep last beam as-is for cheating
    mask = tf.logical_or(mask, mask2) 
    recomb_scores = where_bc(mask, scores, float("-inf"))
    return where_bc(end_flags, scores, recomb_scores)

StoreAlignmentUpToEpoch = 10 * EpochSplit  # 0 based, exclusive
AlignmentFilenamePattern = "net-model/alignments.%i.hdf"


def get_most_recent_align_hdf_files(epoch0):
  """
  :param int epoch0: 0-based (sub) epoch
  :return: filenames or None if there is nothing completed yet
  :rtype: list[str]|None
  """
  if epoch0 < EpochSplit:
    return None
  if epoch0 > StoreAlignmentUpToEpoch:
    epoch0 = StoreAlignmentUpToEpoch  # first epoch after
  i = ((epoch0 - EpochSplit) // EpochSplit) * EpochSplit
  return [AlignmentFilenamePattern % j for j in range(i, i + EpochSplit)]


def segmental_blank_dist(source, **kwargs):
  import tensorflow as tf
  emit_log_prob = source(0, as_data=True, auto_convert=False)   # [B,T',1]
  emit_log_prob_t = tf.squeeze(emit_log_prob.get_placeholder_as_batch_major(), axis=-1)  # [B,T']
  blank_log_prob = source(1, as_data=True, auto_convert=False)  # [B,T',1]
  blank_log_prob_t = tf.squeeze(blank_log_prob.get_placeholder_as_batch_major(), axis=-1)  # [B,T']

  n_batch = emit_log_prob.get_batch_dim()
  max_t = emit_log_prob.time_dimension()  # T', which is T' = T - t_i+1

  # t_dist = [...] ; csum_blank + emit_prob
  # we compute for t= {t_i +1, ..., T}
  csum_blank = tf.concat([
    tf.tile([[0.]], [n_batch, 1]),  # for t_{i-1}+1 we have no contribution from the blank_probs (only emit)
    tf.cumsum(blank_log_prob_t, axis=1)  # inclusive
  ], axis=1)  # [B,T'+1]

  batchs, times = tf.meshgrid(tf.range(n_batch), tf.range(max_t), indexing="ij")  # [B,T']
  batchs1, times1 = tf.meshgrid(tf.range(n_batch), tf.range(max_t+1), indexing="ij")  # [B,T']

  # t_i = t_{i-1}+1 := p(emit)
  emit_prob_idxs = tf.stack([batchs, times], axis=-1)  # [B,T',2]
  emit_probs = tf.gather_nd(emit_log_prob_t, emit_prob_idxs)  # [B,T']

  emit_probs = tf.concat([emit_probs, tf.tile([[0.]], [n_batch, 1])], axis=1)  # [B,T'+1]
  mask = tf.greater_equal(times1, emit_log_prob.get_sequence_lengths()[:, None])
  # for the last item (t_i=T+1), we have no emission.
  emit_probs = tf.where(mask, tf.zeros_like(emit_probs), emit_probs)

  t_dist = emit_probs + csum_blank  # [B,T'+1]
  from returnn.tf.util.basic import filter_ended_scores
  self = kwargs['self']
  search_choice = self._src_common_search_choices
  end_flags = self.network.get_rec_step_info().get_end_flag(search_choice)
  t_dist = filter_ended_scores(t_dist, end_flags, batch_dim=n_batch, dim=None)

  # print_op = tf.print("T'=", emit_log_prob.get_sequence_lengths()[0],
                      # "blank_probs=", blank_log_prob_t[0], "\n\t",
                      # "emit_log_prob_t=", emit_log_prob_t[0], "\n\t",
                      # "mask=", mask[0], "\n\t",
                      # "emit_probs=", emit_probs[0], "\n\t",
                      # "csum_blank=", csum_blank[0], "\n\t",
                      # "t_dist=", t_dist[0], "\n\t",
                      # "sum_t_scores=", tf.exp(tf.reduce_logsumexp(t_dist, axis=1)),
                      # summarize=-1)
  # with tf.control_dependencies([print_op]):
    # t_dist = tf.identity(t_dist)

  return t_dist


# "alpha_t_i_label": {"class": "gather_nd", "from": "label_log_prob", "position": "rel_t_clip"},  # [B,K]
# "y_dist": {"class": "eval", "from": ["alpha_t_i_label", "t", "base:encoder"], "eval": segmental_label_dist,
def segmental_label_dist(source, **kwargs):
  from returnn.tf.compat import v1 as tf
  alpha_t_i_label = source(0, as_data=True, auto_convert=False)   # [B,1030]
  t = source(1, as_data=True, auto_convert=False)  # [B]
  t_i = t.get_placeholder_as_batch_major()
  encoder = source(2, as_data=True, auto_convert=False)  # [B, T, D]
  alpha_t_i_label_t = alpha_t_i_label.get_placeholder_as_batch_major()  # (B, 1030)

  n_batch = alpha_t_i_label.get_batch_dim()

  zero_prob = float("-inf")
  batchs, vocab = tf.meshgrid(tf.range(n_batch), tf.range(target_num_labels), indexing="ij")  # [B,K]

  # This assumes the following vocabulary:
  eos_symbol = 0
  mask_eos_symbol = tf.equal(vocab, eos_symbol)  # [B,K]
  mask_eos_time = tf.greater_equal(t_i, encoder.get_sequence_lengths())
  scores = tf.where(mask_eos_symbol,
                    tf.where(mask_eos_time,
                             tf.zeros_like(alpha_t_i_label_t),  # prob=1 for EOS,
                             tf.ones_like(alpha_t_i_label_t) * zero_prob  # prob=0 for t_i < T
                             ),
                    tf.where(mask_eos_time,
                             tf.ones_like(alpha_t_i_label_t) * zero_prob,
                             alpha_t_i_label_t))  # scores for symbol != EOS, t_i < T

  # print_op = tf.print("t_i=", t_i, ", enc-T=", encoder.get_sequence_lengths(), "\n\t",
                      # "scores=", scores[0, :10], "\n\t",
                      # "scores_sum=", tf.reduce_logsumexp(scores[0, 1:]),
                      # "scores_sum_exp=", tf.exp(tf.reduce_logsumexp(scores[0, 1:])),
                      # "scores_sum_exp_all=", tf.exp(tf.reduce_logsumexp(scores[0])),
                      # "scores_label_emit", tf.exp(tf.reduce_logsumexp(alpha_t_i_label_t)),
                      # summarize=-1)
  # with tf.control_dependencies([print_op]):
      # scores = tf.identity(scores)

  return scores


# "prev_output": {"class": "eval", "from": ["prev:output", "am_window"], "eval": prev_output},
def prev_output(source, **kwargs):
  from returnn.tf.compat import v1 as tf
  prev_output = source(0, as_data=True, auto_convert=False)   # [B] -> [1030]
  am_window = source(1, as_data=True, auto_convert=False)  # [B, T', D]
  t_prime = am_window.time_dimension()
  n_batch = am_window.get_batch_dim()

  # [B, 1] ; [B, T'-1]
  prev_output_t = tf.expand_dims(prev_output.get_placeholder_as_batch_major(), axis=1)
  blank_ones = tf.ones((n_batch, t_prime-1), dtype=tf.int32)*targetb_blank_idx
  out = tf.concat([prev_output_t, blank_ones], axis=1)  # [B,T']
  return out


def search_checks(layer):
    """
    :param ChoiceLayer layer:
    :rtype: list[tf.Operation]
    """
    # if task != "train":
        # return []
    if not layer.search_choices:  # training without search
        return []
    from returnn.tf.compat import v1 as tf
    from TFUtil import get_shape
    from TFNetworkLayer import SelectSearchSourcesLayer
    checks = []
    # without any recombination, we expect all beam entries to be non-inf
    beam_scores = layer.search_choices.beam_scores  # (batch,beam)
    i = layer.network.get_rec_step_index()
    n_batch, n_beam = get_shape(beam_scores)
    label = tf.reshape(layer.output.placeholder, (n_batch, n_beam))  # (batch,beam)
    in_scores = tf.reshape(SelectSearchSourcesLayer.select_if_needed(
        layer.sources[0], search_choices=layer.search_choices).output.placeholder,
        (n_batch, n_beam, -1))  # (batch,beam,dim)
    if layer.name == "rel_t":
        t_layer = layer
    else:
        t_layer = SelectSearchSourcesLayer.select_if_needed(
            layer=layer.network.get_layer("rel_t"),
            search_choices=layer.search_choices)
    t = tf.reshape(t_layer.output.placeholder, (n_batch, n_beam))  # (batch,beam)
    prev_t_layer = SelectSearchSourcesLayer.select_if_needed(
        layer=layer.network.get_layer("prev:t"),
        search_choices=layer.search_choices)
    prev_t = tf.reshape(prev_t_layer.output.placeholder, (n_batch, n_beam))  # (batch,beam)
    # t_base_seq = layer.network.get_layer("base:data:t_base").output.placeholder  # (batch,dec-T)
    # t_base = layer.network.get_layer("data:t_base").output.placeholder  # (batch,)
    enc_seq_lens = layer.network.get_layer("base:encoder").output.get_sequence_lengths()  # (batch,)  
    dec_seq_lens = layer.network.get_layer("base:data:%s" % target).output.get_sequence_lengths()  # (batch,)
    end_flags = tf.reshape(layer.network.get_rec_step_info().get_end_flag(
        target_search_choices=layer.search_choices), (n_batch, n_beam))  # (batch,beam)
    beam_scores_finite = tf.is_finite(beam_scores)
    beam_scores_any_finite = tf.reduce_all(tf.logical_or(beam_scores_finite, end_flags), axis=1)  # (batch,)
    # beam_scores_cheat_finite = beam_scores_finite[:,-1]  # (batch,). the last beam is the cheating beam
    t_check = tf.logical_or(end_flags, tf.greater(t, prev_t))  # (batch,beam)
    t_check = tf.logical_or(t_check, tf.equal(t, enc_seq_lens[:,None] - 1))  # allow to loop in last frame
    t_check = tf.logical_and(tf.reduce_any(t_check, axis=1), t_check[:,-1])  # (batch,)
    bad_batch_idx = tf.argmin(tf.cast(
        tf.logical_and(beam_scores_any_finite, t_check), tf.int32))
    checks.append(tf.print({"beam_scores_finite": tf.logical_or(beam_scores_finite, end_flags)}, summarize=-1))
    checks.append(
        tf.Assert(
            # tf.logical_and(
                # tf.logical_and(tf.reduce_all(beam_scores_any_finite), tf.reduce_all(beam_scores_cheat_finite)),
                tf.reduce_all(beam_scores_any_finite),
                # tf.reduce_all(t_check)),
            ["layer", layer.name, "i", i,
             "any_finite", beam_scores_any_finite,# "cheat_finite", beam_scores_cheat_finite,
             "bad_batch_idx", bad_batch_idx, "n_batch", n_batch, "n_beam", n_beam,
             "beam scores", beam_scores[bad_batch_idx],
             "label", label[bad_batch_idx],
             "prev:t", prev_t[bad_batch_idx],
             "enc_seq_len", enc_seq_lens[bad_batch_idx],
             "dec_seq_len", dec_seq_lens[bad_batch_idx],
             "end_flag", end_flags[bad_batch_idx],
             "scores_in_", in_scores[bad_batch_idx],  # in_scores[bad_batch_idx,-1],
             # "scores_in", layer.search_scores_in[bad_batch_idx, -1],
             "scores_base", layer.search_scores_base[bad_batch_idx],
             "scores_comb", layer.search_scores_combined[bad_batch_idx, -1]],
            summarize=100,
            name="beam_scores_finite_check"))
    return checks




#import_model_train_epoch1 = "base/data-train/base2.conv2l.specaug4a/net-model/network.160"
_train_setup_dir = "data-train/rna3c-lm4a.convtrain.switchout6.l2a_1e_4.nohdf.encbottle256.attwb5_am.dec1la-n128.decdrop03.decwdrop03.pretrain_less2_rep6.mlr50.emit2.fl2.fixmask.rna-align-blank0-scratch-swap.encctc.devtrain.retrain1"
model = _train_setup_dir + "/net-model/network"
preload_from_files = {
  #"base": {
  #  "init_for_train": True,
  #  "ignore_missing": True,
  #  "filename": "/u/zeyer/setups/switchboard/2018-10-02--e2e-bpe1k/data-train/base2.conv2l.specaug4a/net-model/network.160",
  #},
  #"encoder": {
  #  "init_for_train": True,
  #  "ignore_missing": True,
  #  "filename": "/u/zeyer/setups/switchboard/2017-12-11--returnn/data-train/#dropout01.l2_1e_2.6l.n500.inpstddev3.fl2.max_seqs100.grad_noise03.nadam.lr05e_3.nbm6.nbrl.grad_clip_inf.nbm3.run1/net-model/network.077",
  #},
  #"encoder": {
  #  "init_for_train": True,
  #  "ignore_missing": True,
  #  "ignore_params_prefixes": {"output/"},
  #  "filename": "/u/zeyer/setups/switchboard/2019-10-22--e2e-bpe1k/data-train/%s/net-model/network.pretrain.250" % _import_baseline_setup,
  #}
}
#lm_model_filename = "/work/asr3/irie/experiments/lm/switchboard/2018-01-23--lmbpe-zeyer/data-train/bpe1k_clean_i256_m2048_m2048.sgd_b16_lr0_cl2.newbobabs.d0.2/net-model/network.023"




def get_net_dict(pretrain_idx):
  """
  :param int|None pretrain_idx: starts at 0. note that this has a default repetition factor of 6
  :return: net_dict or None if pretrain should stop
  :rtype: dict[str,dict[str]|int]|None
  """
  # Note: epoch0 is 0-based here! I.e. in contrast to elsewhere, where it is 1-based.
  # Also, we never use #repetition here, such that this is correct.
  # This is important because of sub-epochs and storing the HDF files,
  # to know exactly which HDF files cover the dataset completely.
  epoch0 = pretrain_idx
  net_dict = {}

  # network
  # (also defined by num_inputs & num_outputs)
  EncKeyTotalDim = 200
  AttNumHeads = 1  # must be 1 for hard-att
  AttentionDropout = 0.1
  EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads
  EncValueTotalDim = 2048
  EncValuePerHeadDim = EncValueTotalDim // AttNumHeads
  LstmDim = EncValueTotalDim // 2
  l2 = 0.0001

  have_existing_align = True  # only in training, and only in pretrain, and only after the first epoch
  if pretrain_idx is not None:
    net_dict["#config"] = {}

    # Do this in the very beginning.
    #lr_warmup = [0.0] * EpochSplit  # first collect alignments with existing model, no training
    lr_warmup = list(numpy.linspace(learning_rate * 0.1, learning_rate, num=10))
    #lr_warmup += [learning_rate] * 20
    if pretrain_idx < len(lr_warmup):
      net_dict["#config"]["learning_rate"] = lr_warmup[pretrain_idx]
    #if pretrain_idx >= EpochSplit + EpochSplit // 2:
    #    net_dict["#config"]["param_variational_noise"] = 0.1
    #pretrain_idx -= len(lr_warmup)

  use_targetb_search_as_target = False  # not have_existing_align or epoch0 < StoreAlignmentUpToEpoch
  keep_linear_align = False  # epoch0 is not None and epoch0 < EpochSplit * 2

  # We import the model, thus no growing.
  start_num_lstm_layers = 2
  final_num_lstm_layers = 6
  num_lstm_layers = final_num_lstm_layers
  if pretrain_idx is not None:
    pretrain_idx = max(pretrain_idx, 0) // 6  # Repeat a bit.
    num_lstm_layers = pretrain_idx + start_num_lstm_layers
    pretrain_idx = num_lstm_layers - final_num_lstm_layers
    num_lstm_layers = min(num_lstm_layers, final_num_lstm_layers)

  if final_num_lstm_layers > start_num_lstm_layers:
    start_dim_factor = 0.5
    grow_frac = 1.0 - float(final_num_lstm_layers - num_lstm_layers) / (final_num_lstm_layers - start_num_lstm_layers)
    dim_frac = start_dim_factor + (1.0 - start_dim_factor) * grow_frac
  else:
    dim_frac = 1.

  time_reduction = [3, 2] if num_lstm_layers >= 3 else [6]

  if pretrain_idx is not None and pretrain_idx <= 1 and "learning_rate" not in net_dict["#config"]:
    # Fixed learning rate for the beginning.
    net_dict["#config"]["learning_rate"] = learning_rate

  net_dict["#info"] = {
    "epoch0": epoch0,  # Set this here such that a new construction for every pretrain idx is enforced in all cases.
    "num_lstm_layers": num_lstm_layers,
    "dim_frac": dim_frac,
    "have_existing_align": have_existing_align,
    "use_targetb_search_as_target": use_targetb_search_as_target,
    "keep_linear_align": keep_linear_align,
  }

  # We use this pretrain construction during the whole training time (epoch0 > num_epochs).
  if pretrain_idx is not None and epoch0 % EpochSplit == 0 and epoch0 > num_epochs:
    # Stop pretraining now.
    return None

  net_dict.update({
    "source": {"class": "eval", "eval": "self.network.get_config().typed_value('transform')(source(0, as_data=True), network=self.network)"},
    "source0": {"class": "split_dims", "axis": "F", "dims": (-1, 1), "from": "source"},  # (T,40,1)

    # Lingvo: ep.conv_filter_shapes = [(3, 3, 1, 32), (3, 3, 32, 32)],  ep.conv_filter_strides = [(2, 2), (2, 2)]
    "conv0": {"class": "conv", "from": "source0", "padding": "same", "filter_size": (3, 3), "n_out": 32, "activation": None, "with_bias": True},  # (T,40,32)
    "conv0p": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1, 2), "from": "conv0"},  # (T,20,32)
    "conv1": {"class": "conv", "from": "conv0p", "padding": "same", "filter_size": (3, 3), "n_out": 32, "activation": None, "with_bias": True},  # (T,20,32)
    "conv1p": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1, 2), "from": "conv1"},  # (T,10,32)
    "conv_merged": {"class": "merge_dims", "from": "conv1p", "axes": "static"},  # (T,320)

    # Encoder LSTMs added below, resulting in "encoder0".

    #"encoder": {"class": "postfix_in_time", "postfix": 0.0, "from": "encoder0"},
    "encoder": {"class": "linear", "from": "encoder0", "n_out": 256, "activation": None},
    "enc_ctx0": {"class": "linear", "from": "encoder", "activation": None, "with_bias": False, "n_out": EncKeyTotalDim},
    "enc_ctx_win": {"class": "window", "from": "enc_ctx0", "window_size": 5},  # [B,T,W,D]
    "enc_val": {"class": "copy", "from": "encoder"},
    "enc_val_win": {"class": "window", "from": "enc_val", "window_size": 5},  # [B,T,W,D]

    "enc_seq_len": {"class": "length", "from": "encoder", "sparse": True},

    # for task "search" / search_output_layer
    "output_wo_b0": {
      "class": "masked_computation", "unit": {"class": "copy"},
      "from": "output", "mask": "output/output_emit"},
    "output_wo_b": {"class": "reinterpret_data", "from": "output_wo_b0", "set_sparse_dim": target_num_labels},
    "output_": {"class": "reinterpret_data", "from": "output", "set_sparse_dim": target_num_labels},
    "decision": {
      "class": "decide", "from": "output_", "loss": "edit_distance", "target": target,
      'only_on_search': True},

    "targetb_linear": {
      "class": "eval", "from": ["data:%s" % target, "encoder"], "eval": targetb_linear,
      "out_type": targetb_linear_out},

    # Target for decoder ('output') with search ("extra.search") in training.
    # The layer name must be smaller than "t_target" such that this is created first.
    "1_targetb_base": {
      "class": "copy",
      "from": "existing_alignment",  # if have_existing_align else "targetb_linear",
      "register_as_extern_data": "targetb_base" if task == "train" else None},

    "2_targetb_target": {
      "class": "eval",
      "from": "targetb_search_or_fallback" if use_targetb_search_as_target else "data:targetb_base",
      "eval": "source(0)",
      "register_as_extern_data": "targetb" if task == "train" else None},

    "ctc_out": {"class": "softmax", "from": "encoder", "with_bias": False, "n_out": targetb_num_labels},
    #"ctc_out_prior": {"class": "reduce", "mode": "mean", "axes": "bt", "from": "ctc_out"},
    ## log-likelihood: combine out + prior
    "ctc_out_scores": {
      "class": "eval", "from": ["ctc_out"],
      "eval": "safe_log(source(0))",
      #"eval": "safe_log(source(0)) * am_scale - tf.stop_gradient(safe_log(source(1)) * prior_scale)",
      #"eval_locals": {
      #"am_scale": 1.0,  # WrapEpochValue(lambda epoch: numpy.clip(0.05 * epoch, 0.1, 0.3)),
      #"prior_scale": 0.5  # WrapEpochValue(lambda epoch: 0.5 * numpy.clip(0.05 * epoch, 0.1, 0.3))
      #}
    },

    "_target_masked": {"class": "masked_computation",
                       "mask": "output/output_emit",
                       "from": "output",
                       "unit": {"class": "copy"}},
    "3_target_masked": {
      "class": "reinterpret_data", "from": "_target_masked",
      "set_sparse_dim": target_num_labels,  # we masked blank away
      "enforce_batch_major": True,  # ctc not implemented otherwise...
      "register_as_extern_data": "targetb_masked" if task == "train" else None},

    "ctc": {"class": "copy", "from": "ctc_out_scores",
            "loss": "ctc" if task == "train" else None,
            "target": "targetb_masked" if task == "train" else None,
            "loss_opts": {
              "beam_width": 1, "use_native": True, "output_in_log_space": True,
              "ctc_opts": {"logits_normalize": False}} if task == "train" else None
            },
    #"ctc_align": {"class": "forced_align", "from": "ctc_out_scores", "input_type": "log_prob",
    #"align_target": "data:%s" % target, "topology": "ctc"},
  })

  if have_existing_align:
    net_dict.update({
      # This should be compatible to t_linear or t_search.
      "existing_alignment": {
        "class": "reinterpret_data", "from": "data:alignment",
        "set_sparse": True,  # not sure what the HDF gives us
        "set_sparse_dim": targetb_num_labels,
        "size_base": "encoder",  # for RNA...
      },
    })

  # Add encoder BLSTM stack.
  src = "conv_merged"
  if num_lstm_layers >= 1:
    net_dict.update({
      "lstm0_fw": {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2, "direction": 1, "from": src, "trainable": True},
      "lstm0_bw": {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2, "direction": -1, "from": src, "trainable": True}})
    src = ["lstm0_fw", "lstm0_bw"]
  for i in range(1, num_lstm_layers):
    red = time_reduction[i - 1] if (i - 1) < len(time_reduction) else 1
    net_dict.update({
      "lstm%i_pool" % (i - 1): {"class": "pool", "mode": "max", "padding": "same", "pool_size": (red,), "from": src}})
    src = "lstm%i_pool" % (i - 1)
    net_dict.update({
      "lstm%i_fw" % i: {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2,
                        "direction": 1, "from": src, "dropout": 0.3 * dim_frac, "trainable": True},
      "lstm%i_bw" % i: {"class": "rec", "unit": "nativelstm2", "n_out": int(LstmDim * dim_frac), "L2": l2,
                        "direction": -1, "from": src, "dropout": 0.3 * dim_frac, "trainable": True}})
    src = ["lstm%i_fw" % i, "lstm%i_bw" % i]
  net_dict["encoder0"] = {"class": "copy", "from": src}  # dim: EncValueTotalDim

  def get_output_dict(train, search, targetb):
    # labelsync search, see thesis
    return {"class": "rec", "from": [], "back_prop": (task == "train") and train, "include_eos": False,
            "unit": {
              "prev_t_clip": {"class": "eval", "from": ["prev:t", "base:enc_seq_len"],
                              "eval": "tf.maximum(0, tf.minimum(source(0), source(1)-1))"},
              "am_window": {"class": "slice_nd", "from": "base:encoder", "start": "prev_t_clip", "size": None, "min_size": 1},  # [B,T',D]
              "am": {"class": "gather_nd", "from": "base:encoder", "position": "prev_t_clip"},

              "prev_out_non_blank": {
                "class": "reinterpret_data", "from": "prev:output", "set_sparse_dim": target_num_labels},

              # This is just a normal SubnetworkLayer instead of a MaskedComputationLayer
              #                       ^ Label-sync                 ^ Time-sync
              "lm_masked": {"class": "subnetwork", "from": "prev_out_non_blank",
                            "subnetwork": {
                              "input_embed": {"class": "linear", "activation": None, "with_bias": False, "from": "data",
                                              "n_out": 621},
                              "lstm0": {"class": "rec", "unit": "nativelstm2", "n_out": LstmDim,
                                        "from": ["input_embed", "base:am"],
                                        },
                              "output": {"class": "copy", "from": "lstm0"},
                            },
                            },

              "lm": {"class": "reinterpret_data", "from": "lm_masked", "enforce_batch_major": True},
              # "lm_print": {"class": "print", "from": "lm", "filename": "data/step-data/segmental/tensor.lm"},

              # Output:
              # First decoder-step: [0 1030 1030 1030 ...] (length T)
              # After that : [prev:output 1030 1030 1030 ...] (length T')
              # We first feed "0" as the initial output, then blank-symbol
              "prev_output": {"class": "eval", "from": ["prev:output", "am_window"],
                              "eval": prev_output,
                              "out_type": {"batch_dim_axis": 0, "time_dim_axis": 1, "shape": (None,),
                                           "sparse": True, "dtype": "int32"}, "n_out": targetb_num_labels},
              "prev_out_embed": {"class": "linear", "from": "prev_output", "activation": None, "n_out": 128},

              "s_input": {"class": "copy", "from": ["am_window", "prev_out_embed", "lm"]},
              # "s_input": {"class": "print", "from": "s_input0", "filename":
                          # "data/step-data/segmental/tensor.s_input"},
              "s": {"class": "rec", "unit": "NativeLstm2StoreStates", "from": "s_input",
                    "n_out": 128, "L2": l2, "dropout": 0.3,
                    "initial_state": {"c": "prev:s_state_c", "h": "prev:s_state_h"},
                    "unit_opts": {"rec_weight_dropout": 0.3}},  # [B,T',D]

              # We can't use GetLastHiddenStateLayer, because it expects the hidden state to be [B,D],
              # but we have [T,B,D] in this special case.
              "s_states_c": {"class": "eval", "from": "s", "n_out": 128,
                             "eval": "(source(0), self.sources[0].get_last_hidden_state(key='c'))[-1]",
                             "out_type": {"time_dim_axis": 0, "batch_dim_axis": 1, "shape": (None, 128)}},  # [T',B,D]
              "s_states_h": {"class": "eval", "from": "s", "n_out": 128,
                             "eval": "(source(0), self.sources[0].get_last_hidden_state(key='h'))[-1]",
                             "out_type": {"time_dim_axis": 0, "batch_dim_axis": 1, "shape": (None, 128)}},  # [T',B,D]

              "s_state_c": {"class": "gather_nd", "from": "s_states_c", "position": "rel_t_clip",
                            "is_output_layer": True, "initial_output": "zeros"},
              "s_state_h": {"class": "gather_nd", "from": "s_states_h", "position": "rel_t_clip",
                            "is_output_layer": True, "initial_output": "zeros"},

              "s_batch": {"class": "reinterpret_data", "from": "s", "enforce_batch_major": True},  # [B, T_w, D]
              # "s_print": {"class": "print", "from": "s_batch", "filename": "data/step-data/segmental/tensor.s"},
              "readout_in": {"class": "linear", "from": ["s_batch", "am_window", "lm"], "activation": None, "n_out": 1000},  # [B, T_w, D]
              # "readout_in_print": {"class": "print", "from": "readout_in",
              #                      "filename": "data/step-data/segmental/tensor.readout_in"},
              "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": "readout_in"},


              "label_log_prob": {
                "class": "linear", "from": "readout", "activation": "log_softmax", "dropout": 0.3, "n_out": target_num_labels},
              # "label_log_prob_print": {"class": "print", "from": "label_log_prob",
              #                          "filename": "data/step-data/segmental/tensor.label_log_prob"},
              "label_prob": {
                "class": "activation", "from": "label_log_prob", "activation": "exp"},
              "emit_prob0": {"class": "linear", "from": "s_batch", "activation": None, "n_out": 1, "is_output_layer": True},
              "emit_log_prob": {"class": "activation", "from": "emit_prob0", "activation": "log_sigmoid"},
              # "emit_log_prob_print": {"class": "print", "from": "emit_log_prob",
              #                         "filename": "data/step-data/segmental/tensor.emit_log_prob"},
              "blank_log_prob": {"class": "eval", "from": "emit_prob0", "eval": "tf.compat.v1.log_sigmoid(-source(0))"},

              # for segmental model
              "rel_t_dist": {"class": "eval", "from": ["emit_log_prob", "blank_log_prob"], "eval": segmental_blank_dist,
                              "out_type": {"shape": (None,), "batch_dim_axis": 0, "feature_dim_axis": 1}, "n_out": None},
              # "rel_t_dist": {"class": "print", "from": "rel_t_dist0"},  # [B,T'+1]
              "rel_t": {"class": "choice", "from": "rel_t_dist", "input_type": "log_prob",
                         "cheating": False,
                         # "control_dependencies_on_output": search_checks,
                         "keep_beams": True, "target": None, "beam_size": t_beam_size * out_beam_size, "length_normalization": False},  # [B]
              # "rel_t": {"class": "print", "from": "rel_t0"},
              "t": {"class": "eval", "from": ["prev:t", "rel_t"], "eval": "source(0) + source(1) + 1",
                    "initial_output": 0, "out_type": {"dtype": "int32"}},

              # "t": {"class": "print", "from": "t0", "initial_output": 0},
              # "t": {"class": "copy", "from": "t0", "initial_output": 0},

              "t_prime": {"class": "length", "from": "am_window"},  # [B]: T'
              # "t_prime": {"class": "print", "from": "t_prime0"},  # [B]: T'
              # we need to do this, because in the case of t_i=T+1,
              # we would gather for `alpha_t_i_label` beyond the last item. Thus we clip the relative pos.
              # also clip for the case am_window.size=0 -> size-1=-1 (clip to 0)
              "rel_t_clip": {"class": "eval", "from": ["rel_t", "t_prime"],
                              "eval": "tf.maximum(0, tf.minimum(source(0), source(1)-1))"},
              "alpha_t_i_label": {"class": "gather_nd", "from": "label_log_prob", "position": "rel_t_clip"},  # [B,V]

              "y_dist": {"class": "eval", "from": ["alpha_t_i_label", "t", "base:encoder"],
                         "eval": segmental_label_dist,
                         "out_type": {"shape": (target_num_labels,), "dim": target_num_labels}},  # [B,K]

              'output': {
                'class': 'choice', 'target': target, 'beam_size': out_beam_size,
                'from': "y_dist", "input_type": "log_prob",
                "initial_output": 0,
                "cheating": False,
                # "cheating": "exclusive" if task == "train" else None,
                # "control_dependencies_on_output": search_checks,
                # "explicit_search_sources": ["prev:out_str", "prev:output"] if task == "search" else None,
                # "custom_score_combine": targetb_recomb_recog if task == "search" else None
              },
              # we needed to increase the dim so that it matches the previous shapes (from time-sync)
              "output_": {"class": "reinterpret_data", "from": "output",
                          "set_sparse_dim": targetb_num_labels,
                          "initial_output": 0},


              "out_str": {
              "class": "eval", "from": ["prev:out_str", "output"],
              "initial_output": None, "out_type": {"shape": (), "dtype": "string"},
              "eval": out_str},

              # "output_is_not_blank": {"class": "compare", "from": "output_", "value": targetb_blank_idx, "kind": "not_equal", "initial_output": True},
              # "output_is_diff_to_before": {"class": "compare", "from": ["output_", "prev:output_"], "kind": "not_equal"},

              # We allow repetitions of the output label. This "output_emit" is True on the first label but False otherwise, and False on blank.
              # "output_emit": {
              # "class": "eval", "from": ["output_is_not_blank", "output_is_diff_to_before"],
              # "is_output_layer": True, "initial_output": True,
              # "eval": "tf.logical_and(source(0), source(1))"},
              # "output_emit": {"class": "constant", "value": True, "initial_output": True},

              # both should be equivalent
              "end": {"class": "compare", "from": "output", "value": 0},
            },
            "target": [target, "target_rel_t"],
            # "size_target": target,
            "max_seq_len": "max_len_from('encoder')",
            }

  net_dict["output"] = get_output_dict(train=True, search=(task != "train"), targetb="targetb")

  return net_dict


network = get_net_dict(pretrain_idx=None)
search_output_layer = "decision"
debug_print_layer_output_template = True

# debug_print_layer_output_shape = True

# trainer
batching = "random"
# Seq-length 'data' Stats:
#  37867 seqs
#  Mean: 447.397258827
#  Std dev: 350.353162012
#  Min/max: 15 / 2103
# Seq-length 'bpe' Stats:
#  37867 seqs
#  Mean: 14.1077719386
#  Std dev: 13.3402518828
#  Min/max: 2 / 82
log_batch_size = True
batch_size = 10000
max_seqs = 200
#max_seq_length = {"bpe": 75}
_time_red = 6
_chunk_size = 60
chunking = ({
              "data": _chunk_size * _time_red,
              "alignment": _chunk_size,
            }, {
              "data": _chunk_size * _time_red // 2,
              "alignment": _chunk_size // 2,
            })
# chunking_variance ...
# min_chunk_size ...

def custom_construction_algo(idx, net_dict):
  # For debugging, use: python3 ./crnn/Pretrain.py config...
  return get_net_dict(pretrain_idx=idx)

# No repetitions here. We explicitly do that in the construction.
#pretrain = {"copy_param_mode": "subset", "construction_algo": custom_construction_algo}

# import_model_train_epoch1 = "base/data-train/rna3c-lm4a.convtrain.switchout6.l2a_1e_4.nohdf.encbottle256.dec1la-n128.decdrop03.decwdrop03.pretrain_less2_rep6.mlr50.emit2.fl2.rep.fixmask.ctcalignfix-ctcalign-p0-6l.chunk60.encctc.devtrain/net-model/network.pretrain.149"


num_epochs = 150
# model = "net-model/network"
cleanup_old_models = True
gradient_clip = 0
#gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
accum_grad_multiple_step = 2
#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
gradient_noise = 0.0
# lr set above
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_error_measure = "dev_error_output/output_prob"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.7
learning_rate_file = "newbob.data"

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5



