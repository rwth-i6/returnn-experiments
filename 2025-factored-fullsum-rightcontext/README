# Experiment Overview

This experiment consists of training and decoding modules for the factored monophone model trained on LibriSpeech 960h. Decoding is done on dev-other using the official 4gram LM.

## Training

The training process uses the following components:
- TensorFlow Backend: For building and training the neural network.
- [RASR (RWTH ASR)](https://www-i6.informatik.rwth-aachen.de/rwth-asr): For data preparation.
- Extended CUDA Kernel: For optimized computations.

### Configuration Files
1. Data Preparation Config Files: Define settings for preparing the training dataset.
2. [RETURNN Config](https://github.com/rwth-i6/returnn):
   - Includes the network definition.
   - Contains RASR settings for the alignment FSA (Finite State Automaton).
3. Additional Layer and CUDA Code:
   - Includes custom layers.
   - Contains CUDA code required for running factored loss.

## Decoding

The decoding process uses the master branch of [RASR](https://www-i6.informatik.rwth-aachen.de/rwth-asr).

### Configuration Files
- Recognition Config: Specifies parameters for decoding and recognition.

