#!rnn.py

import os
import sys

sys.path.insert(0, os.path.dirname(__file__))


def get_network(epoch, **kwargs):
    from networks import networks_dict

    for epoch_ in sorted(networks_dict.keys(), reverse=True):
        if epoch_ <= epoch:
            return networks_dict[epoch_]
    assert False, "Error, no networks found"


def summary(name, x):
    """
    :param str name:
    :param tf.Tensor x: (batch,time,feature)
    """
    from returnn.tf.compat import v1 as tf

    # tf.summary.image wants [batch_size, height,  width, channels],
    # we have (batch, time, feature).
    img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
    img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
    tf.summary.image(name, img, max_outputs=10)
    tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
    mean = tf.reduce_mean(x)
    tf.summary.scalar("%s_mean" % name, mean)
    stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
    tf.summary.scalar("%s_stddev" % name, stddev)
    tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, batch_axis, axis, pos, max_amount):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int batch_axis:
    :param int axis:
    :param tf.Tensor pos: (batch,)
    :param int|tf.Tensor max_amount: inclusive
    """
    from returnn.tf.compat import v1 as tf

    ndim = x.get_shape().ndims
    n_batch = tf.shape(x)[batch_axis]
    dim = tf.shape(x)[axis]
    amount = tf.random_uniform(
        shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32
    )
    pos2 = tf.minimum(pos + amount, dim)
    idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
    pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
    pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
    cond = tf.logical_and(
        tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc)
    )  # (batch,dim)
    if batch_axis > axis:
        cond = tf.transpose(cond)  # (dim,batch)
    cond = tf.reshape(
        cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)]
    )
    from TFUtil import where_bc

    x = where_bc(cond, 0.0, x)
    return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int batch_axis:
    :param int axis:
    :param int|tf.Tensor min_num:
    :param int|tf.Tensor max_num: inclusive
    :param int|tf.Tensor max_dims: inclusive
    """
    from returnn.tf.compat import v1 as tf

    n_batch = tf.shape(x)[batch_axis]
    if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
        num = min_num
    else:
        num = tf.random_uniform(
            shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32
        )
    # https://github.com/tensorflow/tensorflow/issues/9260
    # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
    z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
    _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
    # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
    # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
    if isinstance(num, int):
        for i in range(num):
            x = _mask(
                x,
                batch_axis=batch_axis,
                axis=axis,
                pos=indices[:, i],
                max_amount=max_dims,
            )
    else:
        _, x = tf.while_loop(
            cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
            body=lambda i, x: (
                i + 1,
                tf.where(
                    tf.less(i, num),
                    _mask(
                        x,
                        batch_axis=batch_axis,
                        axis=axis,
                        pos=indices[:, i],
                        max_amount=max_dims,
                    ),
                    x,
                ),
            ),
            loop_vars=(0, x),
        )
    return x


def transform(data, network, time_factor=1):
    x = data.placeholder
    from returnn.tf.compat import v1 as tf

    # summary("features", x)
    step = network.global_train_step
    step1 = tf.where(tf.greater_equal(step, 1000), 1, 0)
    step2 = tf.where(tf.greater_equal(step, 2000), 1, 0)

    def get_masked():
        x_masked = x
        x_masked = random_mask(
            x_masked,
            batch_axis=data.batch_dim_axis,
            axis=data.time_dim_axis,
            min_num=step1 + step2,
            max_num=tf.maximum(tf.shape(x)[data.time_dim_axis] // 100, 2)
            * (1 + step1 + step2 * 2),
            max_dims=20 // time_factor,
        )
        x_masked = random_mask(
            x_masked,
            batch_axis=data.batch_dim_axis,
            axis=data.feature_dim_axis,
            min_num=step1 + step2,
            max_num=2 + step1 + step2 * 2,
            max_dims=data.dim // 5,
        )
        # summary("features_mask", x_masked)
        return x_masked

    x = network.cond_on_train(get_masked, lambda: x)
    return x


def speed_pert(audio, sample_rate, random_state):
    import librosa

    new_sample_rate = int(sample_rate * (1 + random_state.randint(-1, 2) * 0.1))
    if new_sample_rate != sample_rate:
        audio = librosa.core.resample(
            audio, sample_rate, new_sample_rate, res_type="kaiser_fast"
        )
    return audio


accum_grad_multiple_step = 2
adam = True
batch_size = 2400000
batching = "random"
cleanup_old_models = True
debug_mode = False
debug_print_layer_output_template = True
dev = {
    "class": "MetaDataset",
    "data_map": {
        "audio_features": ("zip_dataset", "data"),
        "bpe_labels": ("zip_dataset", "classes"),
    },
    "datasets": {
        "zip_dataset": {
            "class": "OggZipDataset",
            "path": [
                "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.RvwLniNrgMit/output/out.ogg.zip",
                "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.NSdIHfk1iw2M/output/out.ogg.zip",
            ],
            "use_cache_manager": True,
            "audio": {
                "features": "raw",
                "peak_normalization": True,
                "preemphasis": None,
            },
            "targets": {
                "class": "BytePairEncoding",
                "bpe_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.codes",
                "vocab_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.vocab",
                "unknown_label": None,
                "seq_postfix": [0],
            },
            "segment_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/processing/PipelineJob.gTty7UHs0uBu/output/out",
            "partition_epoch": 1,
            "seq_ordering": "sorted_reverse",
        }
    },
    "seq_order_control_dataset": "zip_dataset",
}
device = "gpu"
eval_datasets = {
    "devtrain": {
        "class": "MetaDataset",
        "data_map": {
            "audio_features": ("zip_dataset", "data"),
            "bpe_labels": ("zip_dataset", "classes"),
        },
        "datasets": {
            "zip_dataset": {
                "class": "OggZipDataset",
                "path": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.Cbboscd6En6A/output/out.ogg.zip",
                "use_cache_manager": True,
                "audio": {
                    "features": "raw",
                    "peak_normalization": True,
                    "preemphasis": None,
                },
                "targets": {
                    "class": "BytePairEncoding",
                    "bpe_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.codes",
                    "vocab_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.vocab",
                    "unknown_label": None,
                    "seq_postfix": [0],
                },
                "segment_file": None,
                "partition_epoch": 1,
                "seq_ordering": "sorted_reverse",
                "fixed_random_subset": 3000,
            }
        },
        "seq_order_control_dataset": "zip_dataset",
    }
}
extern_data = {
    "audio_features": {"available_for_inference": True, "shape": (None, 1), "dim": 1},
    "bpe_labels": {
        "available_for_inference": False,
        "shape": (None,),
        "dim": 10025,
        "sparse": True,
    },
}
gradient_clip = 0.0
gradient_noise = 0.0
learning_rate = 0.0009
learning_rate_control = "constant"
learning_rate_file = "learning_rates"
learning_rates = [
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
    8e-05,
]
log = ["./returnn.log"]
log_batch_size = True
log_verbosity = 5
max_seq_length = {"bpe_labels": 75}
max_seqs = 200
model = "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/training/ReturnnTrainingJob.SAh74CLCNJQi/output/models/epoch"
num_epochs = 2035
optimizer_epsilon = 1e-08
save_interval = 1
search_output_layer = "decision"
target = "classes"
task = "train"
tf_log_memory_usage = True
train = {
    "class": "MetaDataset",
    "data_map": {
        "audio_features": ("zip_dataset", "data"),
        "bpe_labels": ("zip_dataset", "classes"),
    },
    "datasets": {
        "zip_dataset": {
            "class": "OggZipDataset",
            "path": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/returnn/oggzip/BlissToOggZipJob.Cbboscd6En6A/output/out.ogg.zip",
            "use_cache_manager": True,
            "audio": {
                "features": "raw",
                "peak_normalization": True,
                "preemphasis": None,
                "pre_process": speed_pert,
            },
            "targets": {
                "class": "BytePairEncoding",
                "bpe_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.codes",
                "vocab_file": "/u/zeineldeen/setups/librispeech/2022-11-28--conformer-att/work/i6_core/text/label/subword_nmt/train/ReturnnTrainBpeJob.vTq56NZ8STWt/output/bpe.vocab",
                "unknown_label": None,
                "seq_postfix": [0],
            },
            "segment_file": None,
            "partition_epoch": 20,
            "seq_ordering": "laplace:.1000",
            "epoch_wise_filter": {(1, 5): {"max_mean_len": 1000}},
        }
    },
    "seq_order_control_dataset": "zip_dataset",
}
truncation = -1
use_tensorflow = True
config = {}

locals().update(**config)

import sys

sys.setrecursionlimit(3000)


def dynamic_learning_rate(*, network, global_train_step, learning_rate, **kwargs):
    initialLR = 8.999999999999999e-05
    peakLR = 0.0009
    finalLR = 1e-06
    cycleEpoch = 915
    totalEpoch = 2035
    nStep = 1350

    steps = cycleEpoch * nStep
    stepSize = (peakLR - initialLR) / steps
    steps2 = (totalEpoch - 2 * cycleEpoch) * nStep
    stepSize2 = (initialLR - finalLR) / steps2

    import tensorflow as tf

    n = tf.cast(global_train_step, tf.float32)
    return tf.where(
        global_train_step <= steps,
        initialLR + stepSize * n,
        tf.where(
            global_train_step <= 2 * steps,
            peakLR - stepSize * (n - steps),
            tf.maximum(initialLR - stepSize2 * (n - 2 * steps), finalLR),
        ),
    )
