#!crnn/rnn.py
# kate: syntax python;
# see also file:///u/zeyer/setups/quaero-en/training/quaero-train11/50h/ann/2015-07-29--lstm-gt50/config-train/dropout01.3l.n500.custom_lstm.adam.lr1e_3.config

import os
import numpy
from subprocess import check_output

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

_cf_cache = {}

def cf(filename):
	"""Cache manager"""
	if filename in _cf_cache:
		return _cf_cache[filename]
	if check_output(["hostname"]).strip().decode("utf8") in ["sulfid", "zink", "cobalt", "niob"]:
		print("use local file: %s" % filename)
		return filename  # for debugging
	cached_fn = check_output(["cf", filename]).strip().decode("utf8")
	assert os.path.exists(cached_fn)
	_cf_cache[filename] = cached_fn
	return cached_fn

# data
feature_dim = 64  # LogMel 64-dim
channel_num = 3 # d + dd
num_inputs = feature_dim * channel_num
num_outputs = 9001  # CART labels
EpochSplit = 6
context_window = 1

def get_sprint_dataset(data):
    assert data in ["train", "cv"]
    epochSplit = {"train": EpochSplit, "cv": 1}

    # see /u/tuske/work/ASR/switchboard/corpus/readme
    # and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
    files = {}
    files["config"] = "config/training.config"
    files["corpus"] = "/u/corpora/speech/switchboard-1/xml/swb1-all/swb1-all.corpus.gz"
    files["segments"] = "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000"}[data]
    files["features"] = "/u/bozheniuk/setups/switchboard/feature_extraction/cluster_setup/logmel64_30/data/logmel.train.bundle"    
    files["lexicon"] = "/u/tuske/work/ASR/switchboard/corpus/train.lex.v1_0_3.ci.gz"
    files["alignment"] = "dependencies/tuske__2016_01_28__align.combined.train"
    files["cart"] = "/u/tuske/work/ASR/switchboard/initalign/data/%s" % {9001: "cart-9000"}[num_outputs]
    for k, v in sorted(files.items()):
        assert os.path.exists(v), "%s %r does not exist" % (k, v)
    estimated_num_seqs = {"train": 227047, "cv": 3000}  # wc -l segment-file
    # features: /u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.*
    args = [
        "--config=" + files["config"],
        lambda: "--*.corpus.file=" + cf(files["corpus"]),
        lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
        {"train": "--*.corpus.segment-order-shuffle=true", "cv": "--*.corpus.segment-order-sort-by-time-length=true"}[data],
        "--*.state-tying.type=cart",
        lambda: "--*.state-tying.file=" + cf(files["cart"]),
        "--*.trainer-output-dimension=%i" % num_outputs,
        lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
        lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
        lambda: "--*.feature-cache-path=" + cf(files["features"]),
        "--*.log-channel.file=log/crnn.sprint.train-dataset.xml",
        "--*.window-size=1",
        "--*.trainer-output-dimension=%i" % num_outputs]
    return {
        "class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
        "sprintConfigStr": args,
        "partitionEpoch": epochSplit[data],
        "estimated_num_seqs": estimated_num_seqs[data] // (epochSplit[data] or 1),
        "context_window": context_window}

cache_size = "0"

# network
# (also defined by num_inputs & num_outputs)
dropout = 0
L2 = 0.1
filter_size = (3, 3)  # for 2D conv on (time, feature) axes

network = {}
_last = "data"
def add_sequential_layer(name, d, from_=None):
    global _last, network
    assert "from" not in d
    if from_ is not None:
        d["from"] = from_
    else:
        d["from"] = [_last]
    assert name not in network
    network[name] = d
    _last = name       


def fixed_padding(prefix, kernel_size, data_format, conv_time_dim):
    """Pads the input along the spatial dimensions.
    """
    pad_total = kernel_size - 1
    feature_pad_beg = pad_total // 2
    feature_pad_end = pad_total - feature_pad_beg

    time_pad_beg = 0
    time_pad_end = 0
 
    add_sequential_layer(
        "%s_pad" % prefix, {
            "class": "pad", "axes": ("s:0", "s:1"), 
            "padding": [
                (time_pad_beg, time_pad_end), 
                (feature_pad_end, feature_pad_end)]})
    return     
 

def conv2d_fixed_padding(prefix, filters, kernel_size, strides, dilation_rate, 
                         data_format, conv_time_dim, source=None):
    """Strided 2-D convolution with explicit padding."""
    fixed_padding("%s_pad" % prefix, kernel_size, data_format, conv_time_dim)    
        
    padding = 'VALID'
    strides = (1, strides) if conv_time_dim else strides
    filter_size = (kernel_size, kernel_size)
    dilation_rate = (dilation_rate, 1) if conv_time_dim else (1, 1)

    if data_format == 'channels_first':         
        NCHW = True
    else:
        NCHW = False
    add_sequential_layer(
        "%s_conv" % prefix, 
        {"class": "conv", "n_out": filters, "filter_size": filter_size, 
         "auto_use_channel_first": NCHW, 
         "strides": strides, "dilation_rate": dilation_rate, 
         "padding": padding, "activation": None, 
         "with_bias": False, "dropout": 0, 
         "forward_weights_init": "xavier", "L2": L2}, 
        from_=source)
    return "%s_conv" % prefix


def _building_block_v2(prefix, filters, projection_shortcut, strides, 
                       dilation_rate, kernel_size, data_format, conv_time_dim):
    add_sequential_layer("%s_in" % prefix, {"class": "copy"})

    add_sequential_layer("%s_bn1" % prefix, 
        {"class": "batch_norm", "masked_time": False, "fused_bn": True})
    add_sequential_layer("%s_relu1" % prefix, 
        {"class": "activation", "activation": "relu", "batch_norm": False})

    if strides > 1 and conv_time_dim:
        conv2d_fixed_padding(
            prefix=("%s_conv_1" % prefix), filters=filters, kernel_size=kernel_size, 
            strides=1, dilation_rate=dilation_rate, data_format=data_format, 
            conv_time_dim=conv_time_dim)
        add_sequential_layer(
            "%s_stride" % prefix, 
            {"class": "slice", "axis": "s:1", "slice_step": strides})
        dilation_rate *= 2
    else:
        conv2d_fixed_padding(prefix=("%s_conv_1" % prefix), filters=filters, 
                     kernel_size=kernel_size, strides=strides,
                     dilation_rate=dilation_rate, 
                     data_format=data_format, conv_time_dim=conv_time_dim)
                

    add_sequential_layer(
        "%s_bn2" % prefix, 
        {"class": "batch_norm", "masked_time": False, "fused_bn": True})
    add_sequential_layer(
        "%s_relu2" % prefix, 
        {"class": "activation", "activation": "relu", "batch_norm": False})

    conv = conv2d_fixed_padding(
        prefix=("%s_conv_2" % prefix), filters=filters, 
        kernel_size=kernel_size, strides=1,
        dilation_rate=dilation_rate,
        data_format=data_format, conv_time_dim=conv_time_dim)

    crop_lr = filter_size[0] - 1
    crop_left = crop_lr // 2
    crop_right = crop_lr - crop_left

    if conv_time_dim:
      if strides > 1:
        crop = int(crop_left * (dilation_rate/2 + dilation_rate))
      else:
        crop = int(crop_left * 2 * dilation_rate)
      add_sequential_layer(
          "%s_crop" % prefix, 
          {"class": "slice", "axis": "T", "slice_start": crop, "slice_end": -crop}, 
          from_=("%s_relu1" % prefix))
      shortcut = "%s_crop" % prefix
      if projection_shortcut is not None:
        shortcut = projection_shortcut(source=shortcut)
    else:
      crop = crop_left
      add_sequential_layer(
          "%s_crop_1" % prefix, 
          {"class": "slice", "axis": "T", "slice_start": crop, "slice_end": -crop}, 
          from_=("%s_relu1" % prefix))
      shortcut = "%s_crop_1" % prefix            

      if projection_shortcut is not None:
          shortcut = projection_shortcut(source=shortcut)

      add_sequential_layer(
          "%s_crop_2" % prefix, 
          {"class": "slice", "axis": "T", "slice_start": crop, "slice_end": -crop}, 
          from_=shortcut)
      shortcut = "%s_crop_2" % prefix

    add_sequential_layer(
        "%s_out" % prefix, 
        {"class": "combine", "kind": "add"}, 
        from_=[conv, shortcut])
    return


def block_layer(prefix, filters, bottleneck, block_fn, blocks, strides,
                dilation_rate, kernel_size, data_format, conv_time_dim):
    filters_out = filters * 4 if bottleneck else filters
    def projection_shortcut(source=None):
        return conv2d_fixed_padding(
            prefix=("%s_sc" % prefix), filters=filters_out, kernel_size=1, 
            strides=strides, dilation_rate=1, data_format=data_format, 
            conv_time_dim=conv_time_dim, source=source)

    block_fn(
        "%s_0" % prefix, filters, projection_shortcut, strides,
        dilation_rate, kernel_size, data_format, conv_time_dim)
    if strides > 1:
        dilation_rate *= strides
    for i in range(1, blocks):
        block_fn(
            "%s_%i" % (prefix, i), filters, None, 1, dilation_rate, 
            kernel_size, data_format, conv_time_dim)
    return add_sequential_layer("%s_out" % prefix, {"class": "copy"})


def build_resnet():
    resnet_version = 2
    conv_time_dim = True
    bottleneck = False
    num_filters = 64
    first_kernel_size = 5
    kernel_size = 3
    conv_stride = 2
    first_pool_size = (2, 1)
    first_pool_stride = (1, 1)
    block_sizes = [3, 3, 3, 3]
    block_strides = [1, 2, 2, 2]
    block_fn = _building_block_v2
    data_format = 'channels_first'
    pre_activation = resnet_version == 2

    if data_format == 'channels_first':         
        NCHW = True
    else:
        NCHW = False

    if conv_time_dim:
        multiplier = 1 if bottleneck else 2    
        building_block_reduction = multiplier * 2 * (kernel_size // 2)
        total_reduction = 2 * (first_kernel_size // 2)

        dilation_rate_multiplier = 2
        
        for i, bs in enumerate(block_sizes):
            total_reduction += building_block_reduction/multiplier * dilation_rate_multiplier
            dilation_rate_multiplier *= block_strides[i]
            total_reduction += building_block_reduction/multiplier * dilation_rate_multiplier
            total_reduction += building_block_reduction * (bs - 1) * dilation_rate_multiplier

        time_dim_reduction = total_reduction 
        context_window = int(2 * (total_reduction // 2) + 1)

    else:
        time_dim_reduction = 0

    conv2d_fixed_padding(
        prefix="c_init", filters=num_filters,
        kernel_size=first_kernel_size, strides=conv_stride,
        dilation_rate=1,        
        data_format=data_format, conv_time_dim=conv_time_dim)

    dilation_rate = 2

    if resnet_version == 1:
        add_sequential_layer("c_init_bn", {
            "class": "batch_norm", "masked_time": False, "fused_bn": True})
        add_sequential_layer("c_init_relu", {
            "class": "activation", "activation": "relu", "batch_norm": False})                
    for i, num_blocks in enumerate(block_sizes):
        filters = num_filters * (2**i)
        block_layer(
            prefix="c_%i" % i, filters=filters, bottleneck=bottleneck,
            block_fn=block_fn, blocks=num_blocks,
            strides=block_strides[i], dilation_rate=dilation_rate,  
            kernel_size=kernel_size,
            data_format=data_format, conv_time_dim=conv_time_dim)
        dilation_rate *= block_strides[i]
    if pre_activation:
        add_sequential_layer("c_out_bn", {"class": "batch_norm", "masked_time": False, "fused_bn": True})
        add_sequential_layer("c_out_relu", {"class": "activation", "activation": "relu", "batch_norm": False})

    # reduce mean works fatser than avg pool:
    add_sequential_layer("out_pool", {"class": "reduce", "mode": "avg", "axes": ("s:1"), "keep_dims": False})         
    add_sequential_layer("linear", {"class": "linear", "activation": "relu", "L2": L2, "n_out": 2048})
    return context_window  

add_sequential_layer("in0", {"class": "split_dims", "axis": "f", "dims": (feature_dim, -1)}) # output: (batch, time, feature = 64, channel = 3)
context_window = build_resnet()
add_sequential_layer("output", {"class": "softmax", "loss": "ce", "L2": L2, "n_out": num_outputs})

train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")

############## debug stuff

#debug_print_layer_output_template = True  # useful for debugging
#debug_print_layer_output_sizes = True
#debug_print_layer_output_shape = True  # might be useful for debugging
#debug_shell_in_runner = True
#log_batch_size = True
tf_log_memory_usage = True

############## debug stuff

# trainer
batching = "random"
batch_size = 10000
max_seqs = 250
chunking = "100:100"
truncation = -1
num_epochs = 80
#pretrain = "default"
#pretrain_construction_algo = "from_output"
gradient_clip = 0
gradient_noise = 0.0
momentum = 0.99
learning_rate = 1e-6
learning_rate_file = "newbob.data"
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
model = "net-model/network"

cleanup_old_models = True
store_metadata_mod_step = 100

# log
log = "log/crnn.train.log"
log_verbosity = 5

