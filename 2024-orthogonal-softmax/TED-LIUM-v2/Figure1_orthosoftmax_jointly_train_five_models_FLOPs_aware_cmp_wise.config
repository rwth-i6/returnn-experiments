#!rnn.py


import numpy as np

backend = "torch"
batch_size = 2240000
batching = "random"
cache_size = "0"
cleanup_old_models = True
dev = {
    "class": "MetaDataset",
    "datasets": {
        "features": {
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/tedlium2/2024-05-14--independent-softmax/work/i6_core/returnn/hdf/BlissToPcmHDFJob.AltkEvXwM3dF/output/audio.hdf"
            ],
        },
        "targets": {
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/tedlium2/2024-05-14--independent-softmax/work/i6_experiments/users/berger/recipe/returnn/hdf/BlissCorpusToTargetHdfJob.qGsTLu0lnU5n/output/targets.hdf"
            ],
            "partition_epoch": 1,
            "seq_ordering": "sorted",
        },
    },
    "data_map": {"data": ("features", "data"), "targets": ("targets", "data")},
    "seq_order_control_dataset": "targets",
}
device = "gpu"
extern_data = {"data": {"dim": 1}, "targets": {"dim": 79, "sparse": True}}
gradient_clip = 0.0
gradient_noise = 0.0
learning_rate_file = "learning_rates"
learning_rates = [
    4e-06,
    7.6e-06,
    1.1200000000000001e-05,
    1.48e-05,
    1.84e-05,
    2.2e-05,
    2.5600000000000002e-05,
    2.92e-05,
    3.2800000000000004e-05,
    3.6400000000000004e-05,
    4e-05,
    4.36e-05,
    4.720000000000001e-05,
    5.080000000000001e-05,
    5.440000000000001e-05,
    5.800000000000001e-05,
    6.16e-05,
    6.520000000000001e-05,
    6.88e-05,
    7.240000000000001e-05,
    7.6e-05,
    7.960000000000001e-05,
    8.32e-05,
    8.680000000000001e-05,
    9.040000000000002e-05,
    9.400000000000001e-05,
    9.760000000000001e-05,
    0.00010120000000000001,
    0.00010480000000000001,
    0.0001084,
    0.00011200000000000001,
    0.0001156,
    0.00011920000000000001,
    0.0001228,
    0.0001264,
    0.00013,
    0.0001336,
    0.0001372,
    0.0001408,
    0.00014439999999999999,
    0.000148,
    0.0001516,
    0.0001552,
    0.0001588,
    0.0001624,
    0.000166,
    0.0001696,
    0.0001732,
    0.00017680000000000001,
    0.0001804,
    0.000184,
    0.0001876,
    0.0001912,
    0.0001948,
    0.0001984,
    0.000202,
    0.0002056,
    0.00020920000000000002,
    0.0002128,
    0.0002164,
    0.00022,
    0.00022360000000000001,
    0.0002272,
    0.0002308,
    0.0002344,
    0.000238,
    0.00024160000000000002,
    0.0002452,
    0.00024880000000000003,
    0.0002524,
    0.000256,
    0.0002596,
    0.0002632,
    0.00026680000000000003,
    0.0002704,
    0.000274,
    0.0002776,
    0.0002812,
    0.0002848,
    0.0002884,
    0.000292,
    0.00029560000000000003,
    0.0002992,
    0.0003028,
    0.0003064,
    0.00031,
    0.00031360000000000003,
    0.0003172,
    0.0003208,
    0.0003244,
    0.000328,
    0.00033160000000000004,
    0.0003352,
    0.0003388,
    0.00034240000000000003,
    0.000346,
    0.00034960000000000004,
    0.0003532,
    0.0003568,
    0.00036040000000000003,
    0.000364,
    0.0003676,
    0.0003712,
    0.0003748,
    0.00037840000000000004,
    0.000382,
    0.0003856,
    0.00038920000000000003,
    0.0003928,
    0.00039640000000000004,
    0.0004,
    0.00039672727272727277,
    0.00039345454545454547,
    0.0003901818181818182,
    0.0003869090909090909,
    0.00038363636363636367,
    0.00038036363636363636,
    0.0003770909090909091,
    0.0003738181818181818,
    0.00037054545454545456,
    0.00036727272727272726,
    0.000364,
    0.00036072727272727276,
    0.00035745454545454546,
    0.0003541818181818182,
    0.0003509090909090909,
    0.00034763636363636366,
    0.00034436363636363636,
    0.0003410909090909091,
    0.00033781818181818186,
    0.00033454545454545456,
    0.0003312727272727273,
    0.000328,
    0.00032472727272727276,
    0.00032145454545454545,
    0.0003181818181818182,
    0.00031490909090909095,
    0.00031163636363636365,
    0.00030836363636363635,
    0.0003050909090909091,
    0.00030181818181818185,
    0.00029854545454545455,
    0.0002952727272727273,
    0.000292,
    0.00028872727272727275,
    0.00028545454545454544,
    0.0002821818181818182,
    0.00027890909090909095,
    0.00027563636363636364,
    0.00027236363636363634,
    0.0002690909090909091,
    0.00026581818181818184,
    0.00026254545454545454,
    0.0002592727272727273,
    0.00025600000000000004,
    0.00025272727272727274,
    0.00024945454545454544,
    0.0002461818181818182,
    0.0002429090909090909,
    0.00023963636363636364,
    0.00023636363636363636,
    0.0002330909090909091,
    0.00022981818181818184,
    0.00022654545454545456,
    0.00022327272727272728,
    0.00022,
    0.00021672727272727273,
    0.00021345454545454546,
    0.00021018181818181818,
    0.0002069090909090909,
    0.00020363636363636366,
    0.00020036363636363638,
    0.0001970909090909091,
    0.00019381818181818183,
    0.00019054545454545455,
    0.00018727272727272728,
    0.000184,
    0.00018072727272727272,
    0.00017745454545454545,
    0.0001741818181818182,
    0.00017090909090909092,
    0.00016763636363636365,
    0.00016436363636363637,
    0.0001610909090909091,
    0.00015781818181818182,
    0.00015454545454545457,
    0.00015127272727272727,
    0.00014800000000000002,
    0.00014472727272727272,
    0.00014145454545454547,
    0.00013818181818181816,
    0.00013490909090909092,
    0.0001316363636363636,
    0.00012836363636363636,
    0.00012509090909090912,
    0.00012181818181818181,
    0.00011854545454545456,
    0.00011527272727272726,
    0.00011200000000000001,
    0.00010872727272727271,
    0.00010545454545454546,
    0.00010218181818181816,
    9.890909090909091e-05,
    9.563636363636366e-05,
    9.236363636363636e-05,
    8.909090909090911e-05,
    8.58181818181818e-05,
    8.254545454545456e-05,
    7.927272727272725e-05,
    7.6e-05,
    7.27272727272727e-05,
    6.945454545454545e-05,
    6.61818181818182e-05,
    6.29090909090909e-05,
    5.963636363636365e-05,
    5.636363636363635e-05,
    5.30909090909091e-05,
    4.98181818181818e-05,
    4.654545454545455e-05,
    4.3272727272727245e-05,
    4e-05,
    3.8621034482758623e-05,
    3.7242068965517244e-05,
    3.5863103448275864e-05,
    3.4484137931034484e-05,
    3.3105172413793104e-05,
    3.172620689655173e-05,
    3.0347241379310348e-05,
    2.8968275862068968e-05,
    2.7589310344827588e-05,
    2.621034482758621e-05,
    2.4831379310344832e-05,
    2.3452413793103452e-05,
    2.2073448275862072e-05,
    2.0694482758620692e-05,
    1.9315517241379313e-05,
    1.7936551724137933e-05,
    1.6557586206896553e-05,
    1.5178620689655173e-05,
    1.3799655172413793e-05,
    1.2420689655172413e-05,
    1.1041724137931037e-05,
    9.662758620689657e-06,
    8.283793103448274e-06,
    6.904827586206901e-06,
    5.525862068965521e-06,
    4.146896551724141e-06,
    2.7679310344827613e-06,
    1.3889655172413814e-06,
    1e-08,
]
log = ["./returnn.log"]
log_batch_size = True
log_verbosity = 5
max_seqs = 128
model = "/u/jxu/setups/tedlium2/2024-05-14--independent-softmax/work/i6_core/returnn/training/ReturnnTrainingJob.X6hEjDbuRROU/output/models/epoch"
num_epochs = 250
num_inputs = 80
num_outputs = {"targets": 79}
optimizer = {"class": "adamw", "epsilon": 1e-16, "weight_decay": 0.001}
save_interval = 1
target = "targets"
task = "train"
tf_log_memory_usage = True
train = {
    "class": "MetaDataset",
    "datasets": {
        "features": {
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/tedlium2/2024-05-14--independent-softmax/work/i6_core/returnn/hdf/BlissToPcmHDFJob.T3qQ5mfrQwlw/output/audio.hdf"
            ],
        },
        "targets": {
            "class": "HDFDataset",
            "use_cache_manager": True,
            "files": [
                "/u/jxu/setups/tedlium2/2024-05-14--independent-softmax/work/i6_experiments/users/berger/recipe/returnn/hdf/BlissCorpusToTargetHdfJob.bAoRzty8czAI/output/targets.hdf"
            ],
            "partition_epoch": 5,
            "seq_ordering": "laplace:.1000",
        },
    },
    "data_map": {"data": ("features", "data"), "targets": ("targets", "data")},
    "seq_order_control_dataset": "targets",
}
update_on_device = True
window = 1
config = {}

locals().update(**config)

import os
import sys

sys.path.insert(0, "/u/jxu/setups/tedlium2/2024-05-14--independent-softmax/recipe")
from i6_experiments.users.jxu.experiments.ctc.tedlium2.pytorch_networks.independent_softmax.jointly_train_two_models.num_params.conformer_size_384_log_mel_ffn_dim_conv_attn_heads_decomposable_random_pct import (
    ConformerCTCModel,
)
from i6_experiments.users.jxu.experiments.ctc.tedlium2.pytorch_networks.independent_softmax.jointly_train_two_models.num_params.conformer_size_384_log_mel_ffn_dim_conv_attn_heads_decomposable_random_pct import (
    ConformerCTCConfig,
)
from i6_models.primitives.feature_extraction import LogMelFeatureExtractionV1Config
from i6_experiments.users.berger.pytorch.custom_parts.specaugment import (
    SpecaugmentByLengthConfigV1,
)
from i6_models.assemblies.conformer_with_dynamic_model_size.selection_with_ortho_softmax_cmp_wise import (
    ConformerEncoderConfig,
)
from i6_models.parts.frontend.vgg_act import VGG4LayerActFrontendV1Config
from torch.nn.modules.activation import ReLU
from i6_models.parts.frontend.vgg_act import VGG4LayerActFrontendV1
from i6_models.config import ModuleFactoryV1
from i6_models.assemblies.conformer_with_dynamic_model_size.selection_with_ortho_softmax_cmp_wise import (
    ConformerBlockConfig,
)
from i6_models.parts.conformer_structure_prune.feedforward import (
    ConformerPositionwiseFeedForwardV1Config,
)
from torch.nn.modules.activation import SiLU
from i6_models.parts.conformer_structure_prune.mhsa import ConformerMHSAV1Config
from i6_models.parts.conformer_structure_prune.convolution import (
    ConformerConvolutionV1Config,
)
from torch.nn.modules.activation import SiLU
from i6_models_repo.i6_models.parts.conformer.norm import LayerNormNC

cfg = ConformerCTCConfig(
    feature_extraction_cfg=LogMelFeatureExtractionV1Config(
        sample_rate=16000,
        win_size=0.025,
        hop_size=0.01,
        f_min=60,
        f_max=7600,
        min_amp=1e-10,
        num_filters=80,
        center=False,
        n_fft=400,
    ),
    specaugment_cfg=SpecaugmentByLengthConfigV1(
        time_min_num_masks=2,
        time_max_mask_per_n_frames=25,
        time_mask_max_size=20,
        freq_min_num_masks=2,
        freq_max_num_masks=16,
        freq_mask_max_size=8,
    ),
    conformer_cfg=ConformerEncoderConfig(
        num_layers=12,
        frontend=ModuleFactoryV1(
            module_class=VGG4LayerActFrontendV1,
            cfg=VGG4LayerActFrontendV1Config(
                in_features=80,
                conv1_channels=32,
                conv2_channels=64,
                conv3_channels=64,
                conv4_channels=32,
                conv_kernel_size=(3, 3),
                conv_padding=None,
                pool1_kernel_size=(2, 1),
                pool1_stride=(2, 1),
                pool1_padding=None,
                pool2_kernel_size=(2, 1),
                pool2_stride=(2, 1),
                pool2_padding=None,
                activation=ReLU(),
                out_features=384,
            ),
        ),
        block_cfg=ConformerBlockConfig(
            ff_cfg=ConformerPositionwiseFeedForwardV1Config(
                input_dim=384, hidden_dim=1536, dropout=0.2, activation=SiLU()
            ),
            mhsa_cfg=ConformerMHSAV1Config(
                input_dim=384, num_att_heads=6, att_weights_dropout=0.1, dropout=0.2
            ),
            conv_cfg=ConformerConvolutionV1Config(
                channels=384,
                kernel_size=31,
                dropout=0.2,
                activation=SiLU(),
                norm=LayerNormNC((384,), eps=1e-05, elementwise_affine=True),
            ),
            layer_dropout=0.1,
            apply_ff_adaptive_dropout=True,
        ),
        pct_params_set=[0.337, 0.457, 0.578, 0.759, 1],
        softmax_kwargs={
            "softmax_constraint_norm": "L2_norm_sqrt",
            "initial_tau": 1,
            "min_tau": 0.01,
            "tau_annealing": 0.999992,
            "softmax_constraint_loss_scale": "linear_increase",
            "max_softmax_constraint_loss_scale": 1,
            "softmax_constraint_warmup_steps": 225000,
        },
    ),
    target_size=79,
    recog_param_pct=1,
    stage_1_global_steps=225000,
    params_kwargs={
        "num_params": [
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
            3.21,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            0.6966666667,
            2.0675,
            2.0675,
            2.0675,
            2.0675,
        ],
        "rest_params": 110,
        "total_params": 400,
    },
    aux_loss_scales={
        "0.337": "focal_loss",
        "0.457": "focal_loss",
        "0.578": "focal_loss",
        "0.759": "focal_loss",
        "1": 1,
    },
    final_dropout=0.2,
)

model_kwargs = {"cfg": cfg}


def get_model(epoch, step, **kwargs):
    return ConformerCTCModel(epoch=epoch, step=step, **model_kwargs, **kwargs)


from i6_experiments.users.jxu.experiments.ctc.tedlium2.pytorch_networks.independent_softmax.jointly_train_two_models.num_params.conformer_size_384_log_mel_ffn_dim_conv_attn_heads_decomposable_random_pct import (
    train_step,
)
