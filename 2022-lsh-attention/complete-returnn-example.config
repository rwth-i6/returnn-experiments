#!rnn.py

# This is an example config on how to train a Transformer using locality-sensitive-hashing
# in RETURNN.
# See here for more detail: https://github.com/rwth-i6/returnn-experiments/tree/master/2022-lsh-attention
# And here for the paper: https://aclanthology.org/2022.iwslt-1.4.pdf
# Note that you need to use this RETURNN development branch: https://github.com/rwth-i6/returnn/tree/frithjof-self-attention

# To get started, first clone RETURNN from the development branch as well as
# the LSH implementation in https://github.com/rwth-i6/returnn-experiments/tree/master/2022-lsh-attention.
import sys
sys.path.insert(1, "path/to/2022-lsh-attention")  # (if you need to change your path)
from lsh_attention import add_lsh_self_attention_layer, add_lsh_cross_attention_layer

import tensorflow as tf

# set your vocab sizes (data = source, classes = target) and dataset size here
num_outputs = {'data': [19093, 1], 'classes': [19093, 1]}
num_seqs = {'train': 612298}

# link your datasets here (can also use a TranslationDataset in plain text format)
train = {   'class': 'HDFDataset',
    'files': ['train.hdf'],
    'partition_epoch': 4,
    'seq_ordering': 'laplace:612',
    'use_cache_manager': True}
dev = {   'class': 'HDFDataset',
    'files': ['dev.hdf'],
    'partition_epoch': 1,
    'seq_ordering': 'sorted',
    'use_cache_manager': True}

# set your files
log = ['crnn.log']
model = 'models/epoch'
learning_rate_file = 'learning_rates'

# maybe change other training settings
accum_grad_multiple_step = 3
batch_size = (4700 * 2) // 3
batching = 'random'
beam_size = 12
cache_size = '0'
cleanup_old_models = False
debug_add_check_numerics_on_output = True
debug_print_layer_output_template = True
device = 'gpu'
gradient_clip = 0
gradient_noise = 0.0
learning_rate = 0.0001
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_min_num_epochs_per_new_lr = 9
learning_rate_control_relative_error_relative_lr = True
log_verbosity = 5
max_seq_length = 100
max_seqs = 8000
newbob_learning_rate_decay = 0.7
newbob_multi_num_epochs = 9
newbob_multi_update_interval = 1
newbob_relative_error_threshold = 0
num_epochs = 300
optimize_move_layers_out = True
optimizer = {'beta1': 0.9, 'beta2': 0.999, 'class': 'Adam', 'epsilon': 1e-08}
search_output_layer = 'decision'
task = 'train'
tf_log_memory_usage = True
truncation = -1
use_tensorflow = True
window = 1

# this is the model definition of a 6-layer "base" Transformer
# LSH is added afterwards, see below.
network = {   'decision': {'class': 'decide', 'from': ['output'], 'loss': 'edit_distance', 'loss_opts': {}, 'target': 'classes'},
    'enc_01': {'class': 'copy', 'from': ['enc_01_ff_out']},
    'enc_01_ff_conv1': {   'activation': 'relu',
                           'class': 'linear',
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_01_ff_laynorm'],
                           'n_out': 2048,
                           'with_bias': True},
    'enc_01_ff_conv2': {   'activation': None,
                           'class': 'linear',
                           'dropout': 0.2,
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_01_ff_conv1'],
                           'n_out': 512,
                           'with_bias': True},
    'enc_01_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_01_ff_conv2']},
    'enc_01_ff_laynorm': {'class': 'layer_norm', 'from': ['enc_01_self_att_out']},
    'enc_01_ff_out': {'class': 'combine', 'from': ['enc_01_self_att_out', 'enc_01_ff_drop'], 'kind': 'add', 'n_out': 512},
    'enc_01_self_att_att': None,
    'enc_01_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_01_self_att_lin']},
    'enc_01_self_att_laynorm': {'class': 'layer_norm', 'from': ['source_embed']},
    'enc_01_self_att_lin': {   'activation': None,
                               'class': 'linear',
                               'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                               'from': ['enc_01_self_att_att'],
                               'n_out': 512,
                               'with_bias': False},
    'enc_01_self_att_out': {'class': 'combine', 'from': ['source_embed', 'enc_01_self_att_drop'], 'kind': 'add', 'n_out': 512},
    'enc_02': {'class': 'copy', 'from': ['enc_02_ff_out']},
    'enc_02_ff_conv1': {   'activation': 'relu',
                           'class': 'linear',
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_02_ff_laynorm'],
                           'n_out': 2048,
                           'with_bias': True},
    'enc_02_ff_conv2': {   'activation': None,
                           'class': 'linear',
                           'dropout': 0.2,
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_02_ff_conv1'],
                           'n_out': 512,
                           'with_bias': True},
    'enc_02_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_02_ff_conv2']},
    'enc_02_ff_laynorm': {'class': 'layer_norm', 'from': ['enc_02_self_att_out']},
    'enc_02_ff_out': {'class': 'combine', 'from': ['enc_02_self_att_out', 'enc_02_ff_drop'], 'kind': 'add', 'n_out': 512},
    'enc_02_self_att_att': None,
    'enc_02_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_02_self_att_lin']},
    'enc_02_self_att_laynorm': {'class': 'layer_norm', 'from': ['enc_01']},
    'enc_02_self_att_lin': {   'activation': None,
                               'class': 'linear',
                               'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                               'from': ['enc_02_self_att_att'],
                               'n_out': 512,
                               'with_bias': False},
    'enc_02_self_att_out': {'class': 'combine', 'from': ['enc_01', 'enc_02_self_att_drop'], 'kind': 'add', 'n_out': 512},
    'enc_03': {'class': 'copy', 'from': ['enc_03_ff_out']},
    'enc_03_ff_conv1': {   'activation': 'relu',
                           'class': 'linear',
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_03_ff_laynorm'],
                           'n_out': 2048,
                           'with_bias': True},
    'enc_03_ff_conv2': {   'activation': None,
                           'class': 'linear',
                           'dropout': 0.2,
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_03_ff_conv1'],
                           'n_out': 512,
                           'with_bias': True},
    'enc_03_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_03_ff_conv2']},
    'enc_03_ff_laynorm': {'class': 'layer_norm', 'from': ['enc_03_self_att_out']},
    'enc_03_ff_out': {'class': 'combine', 'from': ['enc_03_self_att_out', 'enc_03_ff_drop'], 'kind': 'add', 'n_out': 512},
    'enc_03_self_att_att': None,
    'enc_03_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_03_self_att_lin']},
    'enc_03_self_att_laynorm': {'class': 'layer_norm', 'from': ['enc_02']},
    'enc_03_self_att_lin': {   'activation': None,
                               'class': 'linear',
                               'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                               'from': ['enc_03_self_att_att'],
                               'n_out': 512,
                               'with_bias': False},
    'enc_03_self_att_out': {'class': 'combine', 'from': ['enc_02', 'enc_03_self_att_drop'], 'kind': 'add', 'n_out': 512},
    'enc_04': {'class': 'copy', 'from': ['enc_04_ff_out']},
    'enc_04_ff_conv1': {   'activation': 'relu',
                           'class': 'linear',
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_04_ff_laynorm'],
                           'n_out': 2048,
                           'with_bias': True},
    'enc_04_ff_conv2': {   'activation': None,
                           'class': 'linear',
                           'dropout': 0.2,
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_04_ff_conv1'],
                           'n_out': 512,
                           'with_bias': True},
    'enc_04_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_04_ff_conv2']},
    'enc_04_ff_laynorm': {'class': 'layer_norm', 'from': ['enc_04_self_att_out']},
    'enc_04_ff_out': {'class': 'combine', 'from': ['enc_04_self_att_out', 'enc_04_ff_drop'], 'kind': 'add', 'n_out': 512},
    'enc_04_self_att_att': None,
    'enc_04_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_04_self_att_lin']},
    'enc_04_self_att_laynorm': {'class': 'layer_norm', 'from': ['enc_03']},
    'enc_04_self_att_lin': {   'activation': None,
                               'class': 'linear',
                               'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                               'from': ['enc_04_self_att_att'],
                               'n_out': 512,
                               'with_bias': False},
    'enc_04_self_att_out': {'class': 'combine', 'from': ['enc_03', 'enc_04_self_att_drop'], 'kind': 'add', 'n_out': 512},
    'enc_05': {'class': 'copy', 'from': ['enc_05_ff_out']},
    'enc_05_ff_conv1': {   'activation': 'relu',
                           'class': 'linear',
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_05_ff_laynorm'],
                           'n_out': 2048,
                           'with_bias': True},
    'enc_05_ff_conv2': {   'activation': None,
                           'class': 'linear',
                           'dropout': 0.2,
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_05_ff_conv1'],
                           'n_out': 512,
                           'with_bias': True},
    'enc_05_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_05_ff_conv2']},
    'enc_05_ff_laynorm': {'class': 'layer_norm', 'from': ['enc_05_self_att_out']},
    'enc_05_ff_out': {'class': 'combine', 'from': ['enc_05_self_att_out', 'enc_05_ff_drop'], 'kind': 'add', 'n_out': 512},
    'enc_05_self_att_att': None,
    'enc_05_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_05_self_att_lin']},
    'enc_05_self_att_laynorm': {'class': 'layer_norm', 'from': ['enc_04']},
    'enc_05_self_att_lin': {   'activation': None,
                               'class': 'linear',
                               'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                               'from': ['enc_05_self_att_att'],
                               'n_out': 512,
                               'with_bias': False},
    'enc_05_self_att_out': {'class': 'combine', 'from': ['enc_04', 'enc_05_self_att_drop'], 'kind': 'add', 'n_out': 512},
    'enc_06': {'class': 'copy', 'from': ['enc_06_ff_out']},
    'enc_06_ff_conv1': {   'activation': 'relu',
                           'class': 'linear',
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_06_ff_laynorm'],
                           'n_out': 2048,
                           'with_bias': True},
    'enc_06_ff_conv2': {   'activation': None,
                           'class': 'linear',
                           'dropout': 0.2,
                           'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                           'from': ['enc_06_ff_conv1'],
                           'n_out': 512,
                           'with_bias': True},
    'enc_06_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_06_ff_conv2']},
    'enc_06_ff_laynorm': {'class': 'layer_norm', 'from': ['enc_06_self_att_out']},
    'enc_06_ff_out': {'class': 'combine', 'from': ['enc_06_self_att_out', 'enc_06_ff_drop'], 'kind': 'add', 'n_out': 512},
    'enc_06_self_att_att': None,
    'enc_06_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['enc_06_self_att_lin']},
    'enc_06_self_att_laynorm': {'class': 'layer_norm', 'from': ['enc_05']},
    'enc_06_self_att_lin': {   'activation': None,
                               'class': 'linear',
                               'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                               'from': ['enc_06_self_att_att'],
                               'n_out': 512,
                               'with_bias': False},
    'enc_06_self_att_out': {'class': 'combine', 'from': ['enc_05', 'enc_06_self_att_drop'], 'kind': 'add', 'n_out': 512},
    'encoder': {'class': 'layer_norm', 'from': ['enc_06']},
    'output': {   'class': 'rec',
                  'from': [],
                  'max_seq_len': "max_len_from('base:encoder') * 3",
                  'target': 'classes',
                  'unit': {   'dec_01': {'class': 'copy', 'from': ['dec_01_ff_out']},
                              'dec_01_att_att': None,
                              'dec_01_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_01_att_lin']},
                              'dec_01_att_laynorm': {'class': 'layer_norm', 'from': ['dec_01_self_att_out']},
                              'dec_01_att_lin': {   'activation': None,
                                                    'class': 'linear',
                                                    'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                            'scale=1.0)',
                                                    'from': ['dec_01_att_att'],
                                                    'n_out': 512,
                                                    'with_bias': False},
                              'dec_01_att_out': {'class': 'combine', 'from': ['dec_01_self_att_out', 'dec_01_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_01_ff_conv1': {   'activation': 'relu',
                                                     'class': 'linear',
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_01_ff_laynorm'],
                                                     'n_out': 2048,
                                                     'with_bias': True},
                              'dec_01_ff_conv2': {   'activation': None,
                                                     'class': 'linear',
                                                     'dropout': 0.2,
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_01_ff_conv1'],
                                                     'n_out': 512,
                                                     'with_bias': True},
                              'dec_01_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_01_ff_conv2']},
                              'dec_01_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_01_att_out']},
                              'dec_01_ff_out': {'class': 'combine', 'from': ['dec_01_att_out', 'dec_01_ff_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_01_self_att_att': None,
                              'dec_01_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_01_self_att_lin']},
                              'dec_01_self_att_laynorm': {'class': 'layer_norm', 'from': ['target_embed']},
                              'dec_01_self_att_lin': {   'activation': None,
                                                         'class': 'linear',
                                                         'forward_weights_init': "variance_scaling_initializer(mode='fan_in', "
                                                                                 "distribution='uniform', scale=1.0)",
                                                         'from': ['dec_01_self_att_att'],
                                                         'n_out': 512,
                                                         'with_bias': False},
                              'dec_01_self_att_out': {   'class': 'combine',
                                                         'from': ['target_embed', 'dec_01_self_att_drop'],
                                                         'kind': 'add',
                                                         'n_out': 512},
                              'dec_02': {'class': 'copy', 'from': ['dec_02_ff_out']},
                              'dec_02_att_att': None,
                              'dec_02_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_02_att_lin']},
                              'dec_02_att_laynorm': {'class': 'layer_norm', 'from': ['dec_02_self_att_out']},
                              'dec_02_att_lin': {   'activation': None,
                                                    'class': 'linear',
                                                    'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                            'scale=1.0)',
                                                    'from': ['dec_02_att_att'],
                                                    'n_out': 512,
                                                    'with_bias': False},
                              'dec_02_att_out': {'class': 'combine', 'from': ['dec_02_self_att_out', 'dec_02_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_02_ff_conv1': {   'activation': 'relu',
                                                     'class': 'linear',
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_02_ff_laynorm'],
                                                     'n_out': 2048,
                                                     'with_bias': True},
                              'dec_02_ff_conv2': {   'activation': None,
                                                     'class': 'linear',
                                                     'dropout': 0.2,
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_02_ff_conv1'],
                                                     'n_out': 512,
                                                     'with_bias': True},
                              'dec_02_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_02_ff_conv2']},
                              'dec_02_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_02_att_out']},
                              'dec_02_ff_out': {'class': 'combine', 'from': ['dec_02_att_out', 'dec_02_ff_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_02_self_att_att': None,
                              'dec_02_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_02_self_att_lin']},
                              'dec_02_self_att_laynorm': {'class': 'layer_norm', 'from': ['dec_01']},
                              'dec_02_self_att_lin': {   'activation': None,
                                                         'class': 'linear',
                                                         'forward_weights_init': "variance_scaling_initializer(mode='fan_in', "
                                                                                 "distribution='uniform', scale=1.0)",
                                                         'from': ['dec_02_self_att_att'],
                                                         'n_out': 512,
                                                         'with_bias': False},
                              'dec_02_self_att_out': {'class': 'combine', 'from': ['dec_01', 'dec_02_self_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_03': {'class': 'copy', 'from': ['dec_03_ff_out']},
                              'dec_03_att_att': None,
                              'dec_03_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_03_att_lin']},
                              'dec_03_att_laynorm': {'class': 'layer_norm', 'from': ['dec_03_self_att_out']},
                              'dec_03_att_lin': {   'activation': None,
                                                    'class': 'linear',
                                                    'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                            'scale=1.0)',
                                                    'from': ['dec_03_att_att'],
                                                    'n_out': 512,
                                                    'with_bias': False},
                              'dec_03_att_out': {'class': 'combine', 'from': ['dec_03_self_att_out', 'dec_03_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_03_ff_conv1': {   'activation': 'relu',
                                                     'class': 'linear',
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_03_ff_laynorm'],
                                                     'n_out': 2048,
                                                     'with_bias': True},
                              'dec_03_ff_conv2': {   'activation': None,
                                                     'class': 'linear',
                                                     'dropout': 0.2,
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_03_ff_conv1'],
                                                     'n_out': 512,
                                                     'with_bias': True},
                              'dec_03_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_03_ff_conv2']},
                              'dec_03_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_03_att_out']},
                              'dec_03_ff_out': {'class': 'combine', 'from': ['dec_03_att_out', 'dec_03_ff_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_03_self_att_att': None,
                              'dec_03_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_03_self_att_lin']},
                              'dec_03_self_att_laynorm': {'class': 'layer_norm', 'from': ['dec_02']},
                              'dec_03_self_att_lin': {   'activation': None,
                                                         'class': 'linear',
                                                         'forward_weights_init': "variance_scaling_initializer(mode='fan_in', "
                                                                                 "distribution='uniform', scale=1.0)",
                                                         'from': ['dec_03_self_att_att'],
                                                         'n_out': 512,
                                                         'with_bias': False},
                              'dec_03_self_att_out': {'class': 'combine', 'from': ['dec_02', 'dec_03_self_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_04': {'class': 'copy', 'from': ['dec_04_ff_out']},
                              'dec_04_att_att': None,
                              'dec_04_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_04_att_lin']},
                              'dec_04_att_laynorm': {'class': 'layer_norm', 'from': ['dec_04_self_att_out']},
                              'dec_04_att_lin': {   'activation': None,
                                                    'class': 'linear',
                                                    'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                            'scale=1.0)',
                                                    'from': ['dec_04_att_att'],
                                                    'n_out': 512,
                                                    'with_bias': False},
                              'dec_04_att_out': {'class': 'combine', 'from': ['dec_04_self_att_out', 'dec_04_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_04_ff_conv1': {   'activation': 'relu',
                                                     'class': 'linear',
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_04_ff_laynorm'],
                                                     'n_out': 2048,
                                                     'with_bias': True},
                              'dec_04_ff_conv2': {   'activation': None,
                                                     'class': 'linear',
                                                     'dropout': 0.2,
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_04_ff_conv1'],
                                                     'n_out': 512,
                                                     'with_bias': True},
                              'dec_04_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_04_ff_conv2']},
                              'dec_04_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_04_att_out']},
                              'dec_04_ff_out': {'class': 'combine', 'from': ['dec_04_att_out', 'dec_04_ff_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_04_self_att_att': None,
                              'dec_04_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_04_self_att_lin']},
                              'dec_04_self_att_laynorm': {'class': 'layer_norm', 'from': ['dec_03']},
                              'dec_04_self_att_lin': {   'activation': None,
                                                         'class': 'linear',
                                                         'forward_weights_init': "variance_scaling_initializer(mode='fan_in', "
                                                                                 "distribution='uniform', scale=1.0)",
                                                         'from': ['dec_04_self_att_att'],
                                                         'n_out': 512,
                                                         'with_bias': False},
                              'dec_04_self_att_out': {'class': 'combine', 'from': ['dec_03', 'dec_04_self_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_05': {'class': 'copy', 'from': ['dec_05_ff_out']},
                              'dec_05_att_att': None,
                              'dec_05_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_05_att_lin']},
                              'dec_05_att_laynorm': {'class': 'layer_norm', 'from': ['dec_05_self_att_out']},
                              'dec_05_att_lin': {   'activation': None,
                                                    'class': 'linear',
                                                    'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                            'scale=1.0)',
                                                    'from': ['dec_05_att_att'],
                                                    'n_out': 512,
                                                    'with_bias': False},
                              'dec_05_att_out': {'class': 'combine', 'from': ['dec_05_self_att_out', 'dec_05_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_05_ff_conv1': {   'activation': 'relu',
                                                     'class': 'linear',
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_05_ff_laynorm'],
                                                     'n_out': 2048,
                                                     'with_bias': True},
                              'dec_05_ff_conv2': {   'activation': None,
                                                     'class': 'linear',
                                                     'dropout': 0.2,
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_05_ff_conv1'],
                                                     'n_out': 512,
                                                     'with_bias': True},
                              'dec_05_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_05_ff_conv2']},
                              'dec_05_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_05_att_out']},
                              'dec_05_ff_out': {'class': 'combine', 'from': ['dec_05_att_out', 'dec_05_ff_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_05_self_att_att': None,
                              'dec_05_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_05_self_att_lin']},
                              'dec_05_self_att_laynorm': {'class': 'layer_norm', 'from': ['dec_04']},
                              'dec_05_self_att_lin': {   'activation': None,
                                                         'class': 'linear',
                                                         'forward_weights_init': "variance_scaling_initializer(mode='fan_in', "
                                                                                 "distribution='uniform', scale=1.0)",
                                                         'from': ['dec_05_self_att_att'],
                                                         'n_out': 512,
                                                         'with_bias': False},
                              'dec_05_self_att_out': {'class': 'combine', 'from': ['dec_04', 'dec_05_self_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_06': {'class': 'copy', 'from': ['dec_06_ff_out']},
                              'dec_06_att_att': None,
                              'dec_06_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_06_att_lin']},
                              'dec_06_att_laynorm': {'class': 'layer_norm', 'from': ['dec_06_self_att_out']},
                              'dec_06_att_lin': {   'activation': None,
                                                    'class': 'linear',
                                                    'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                            'scale=1.0)',
                                                    'from': ['dec_06_att_att'],
                                                    'n_out': 512,
                                                    'with_bias': False},
                              'dec_06_att_out': {'class': 'combine', 'from': ['dec_06_self_att_out', 'dec_06_att_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_06_ff_conv1': {   'activation': 'relu',
                                                     'class': 'linear',
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_06_ff_laynorm'],
                                                     'n_out': 2048,
                                                     'with_bias': True},
                              'dec_06_ff_conv2': {   'activation': None,
                                                     'class': 'linear',
                                                     'dropout': 0.2,
                                                     'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                             'scale=1.0)',
                                                     'from': ['dec_06_ff_conv1'],
                                                     'n_out': 512,
                                                     'with_bias': True},
                              'dec_06_ff_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_06_ff_conv2']},
                              'dec_06_ff_laynorm': {'class': 'layer_norm', 'from': ['dec_06_att_out']},
                              'dec_06_ff_out': {'class': 'combine', 'from': ['dec_06_att_out', 'dec_06_ff_drop'], 'kind': 'add', 'n_out': 512},
                              'dec_06_self_att_att': None,
                              'dec_06_self_att_drop': {'class': 'dropout', 'dropout': 0.2, 'from': ['dec_06_self_att_lin']},
                              'dec_06_self_att_laynorm': {'class': 'layer_norm', 'from': ['dec_05']},
                              'dec_06_self_att_lin': {   'activation': None,
                                                         'class': 'linear',
                                                         'forward_weights_init': "variance_scaling_initializer(mode='fan_in', "
                                                                                 "distribution='uniform', scale=1.0)",
                                                         'from': ['dec_06_self_att_att'],
                                                         'n_out': 512,
                                                         'with_bias': False},
                              'dec_06_self_att_out': {'class': 'combine', 'from': ['dec_05', 'dec_06_self_att_drop'], 'kind': 'add', 'n_out': 512},
                              'decoder': {'class': 'layer_norm', 'from': ['dec_06']},
                              'end': {'class': 'compare', 'from': ['output'], 'value': 0},
                              'output': {'beam_size': 12, 'class': 'choice', 'from': ['output_prob'], 'initial_output': 0, 'target': 'classes'},
                              'output_prob': {   'class': 'softmax',
                                                 'dropout': 0.0,
                                                 'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                         'scale=1.0)',
                                                 'from': ['decoder'],
                                                 'loss': 'ce',
                                                 'loss_opts': {'label_smoothing': 0.1, 'use_normalized_loss': True},
                                                 'reuse_params': {'map': {'W': {'custom': lambda reuse_layer, **kwargs: tf.transpose(reuse_layer.params["W"]), 'reuse_layer': 'target_embed_raw'}, 'b': None}},
                                                 'target': 'classes',
                                                 'with_bias': True},
                              'target_embed': {'class': 'dropout', 'dropout': 0.2, 'from': ['target_embed_with_pos']},
                              'target_embed_raw': {   'activation': None,
                                                      'class': 'linear',
                                                      'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', "
                                                                              'scale=1.0)',
                                                      'from': ['prev:output'],
                                                      'n_out': 512,
                                                      'reuse_params': {'map': {'W': {'reuse_layer': 'base:source_embed_raw'}, 'b': None}},
                                                      'with_bias': False},
                              'target_embed_weighted': {'class': 'eval', 'eval': 'source(0) * 22.627417', 'from': ['target_embed_raw']},
                              'target_embed_with_pos': {'add_to_input': True, 'class': 'positional_encoding', 'from': ['target_embed_weighted']}}},
    'source_embed': {'class': 'dropout', 'dropout': 0.2, 'from': ['source_embed_with_pos']},
    'source_embed_raw': {   'activation': None,
                            'class': 'linear',
                            'forward_weights_init': "variance_scaling_initializer(mode='fan_in', distribution='uniform', scale=1.0)",
                            'n_out': 512,
                            'with_bias': False},
    'source_embed_weighted': {'class': 'eval', 'eval': 'source(0) * 22.627417', 'from': ['source_embed_raw']},
    'source_embed_with_pos': {'add_to_input': True, 'class': 'positional_encoding', 'from': ['source_embed_weighted']}}




# add LSH attention for all layers
# change parameters however you want
for num_layer in range(1, 6 + 1):
  # encoder self-attention
  add_lsh_self_attention_layer(
    network, input=f"enc_{num_layer:02}_self_att_laynorm", output="enc_{num_layer:02}_self_att",
    allow_duplicate_attention=True, chunk_alignment='identity', chunk_size=6, chunks_after=1, chunks_before=1,
    dropout=0.2, inside_rec_layer=False, key_dim=64,
    num_hashes=4, num_heads=8, num_rounds=4, past_only=False, value_dim=64)
  # decoder self-attention
  add_lsh_self_attention_layer(
    network["output"]["unit"], input=f"dec_{num_layer:02}_self_att_laynorm", output=f"dec_{num_layer:02}_self_att",
    allow_duplicate_attention=True, chunk_alignment='identity', chunk_size=6, chunks_after=1, chunks_before=2,
    dropout=0.2, inside_rec_layer=True, key_dim=64,
    num_hashes=4, num_heads=8, num_rounds=4, past_only=True, value_dim=64)
  # cross-attention
  add_lsh_cross_attention_layer(
    d=network["output"]["unit"], db=network, input=f"dec_{num_layer:02}_att_laynorm", keys_input="base:encoder", output=f"dec_{num_layer:02}_att",
    allow_duplicate_attention=True, chunk_alignment='search_bounds_centered',
    dropout=0.2, key_chunk_size=6, key_chunks_after=None, key_chunks_before=None, key_dim=64, mask_different_hashes=True,
    num_hashes=4, num_heads=8, num_rounds=4, query_chunk_size=6, shuffle_kv=True, value_dim=64)

  # to swap out an attention component with vanilla attention, simply call
  # add_vanilla_self_attention_layer / add_vanilla_cross_attention_layer from vanilla_attention.py
  # instead of the LSH functions.
