#!crnn/rnn.py
# kate: syntax python;

import os
from subprocess import check_output

# task
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
num_inputs = 45  # After LDA 45-dim
num_outputs = 1501 # 4501
EpochSplit = 6

_cf_cache = {}

def cf(filename):
	"""Cache manager"""
	if filename in _cf_cache:
		return _cf_cache[filename]
	if check_output(["hostname"]).strip() in ["cluster-cn-211", "nylon", "blei"]:
		print("use local file: %s" % filename)
		return filename  # for debugging
	cached_fn = check_output(["cf", filename]).strip()
	assert os.path.exists(cached_fn)
	_cf_cache[filename] = cached_fn
	return cached_fn

commonfiles = {
	"corpus": "/u/corpora/speech/CHiME3/corpora/tr05_multi_enhanced-paderborn.corpus",
	"features": "/work/asr2/kitza/experiments/01-features/mfcc-16/chime3/results/mfcc-16.tr05_multi_enhanced-paderborn.bundle",
	"energies": "/work/asr2/kitza/experiments/01-features/mfcc-16/chime3/results/energy.tr05_multi_enhanced-paderborn.bundle",
	"lexicon": "/u/corpora/speech/CHiME3/wsj01-train.lexicon",
	"alignment": "deps/alignment.bundle",
	"cart": "deps/cart.tree.xml.gz"
}

def get_sprint_dataset(data):
	assert data in ["train", "cv"]
	epochSplit = {"train": EpochSplit, "cv": 1}

	files = {
		"config": "config/training.crnn.config",
		"segments": "deps/seg_%s" % {"train":"train", "cv":"cv"}[data]
	}
	files.update(commonfiles)
	for k, v in sorted(files.items()):
		assert os.path.exists(v), "%s %r does not exist" % (k, v)
	estimated_num_seqs = {"train": 6991, "cv": 1747}  # wc -l segment-file

	args = [
	"--config=" + files["config"],
	lambda: "--*.corpus.file=" + cf(files["corpus"]),
	lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
	"--*.corpus.segment-order-shuffle=true",
	"--*.state-tying.type=cart",
	lambda: "--*.state-tying.file=" + cf(files["cart"]),
	"--*.trainer-output-dimension=%i" % num_outputs,
	lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
	lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
	lambda: "--*.feature-cache-path=" + cf(files["features"]),
	lambda: "--*.energy-cache-path=" + cf(files["energies"]),
	"--*.log-channel.file=log/crnn.sprint.train-dataset.log",
	"--*.window-size=1"
	]
	return {
	"class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint/nn-trainer",
	"sprintConfigStr": args,
	"partitionEpoch": epochSplit[data],
	"estimated_num_seqs": estimated_num_seqs[data] // (epochSplit[data] or 1)}

lattice_cache = "lt-ch3-setups/ce.lstm.3l.rmsprop.momentum.lr_1e-4.baseline.bbigger"

def get_sprint_lattice_proc_args():
	files = commonfiles.copy()
	files["mixtureset"] = "deps/split-8.sat.mix"
	files["alignment"] = "%s/data-recog/seq-train-cache/segmentwise-alignment.train" % lattice_cache
	files["lattices"] = "%s/data-recog/seq-train-lattices/state-accuracy.train" % lattice_cache
	for k, v in sorted(files.items()):
		assert os.path.exists(v), "%s %r does not exist" % (k, v)
	return [
		"--config=config/seq-train.train.config",
		lambda: "--*.corpus.file=" + cf(files["corpus"]),
		lambda: "--*.state-tying.file=" + cf(files["cart"]),
		lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
		lambda: "--*.mixture-set.file=" + cf(files["mixtureset"]),
		lambda: "--*.feature-cache-path=" + cf(files["features"]),
		lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
		lambda: "--*.denominator-cache-path=" + cf(files["lattices"]),
		"--*.number-of-classes=%i" % num_outputs
	]

train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
num_inputs = 45  # LDA 45-dim
num_outputs = 1501
network = {
"lstm0_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1 },
"lstm0_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1 },

"lstm1_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm0_fw", "lstm0_bw"] },
"lstm1_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm0_fw", "lstm0_bw"] },

"lstm2_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": 1, "from" : ["lstm1_fw", "lstm1_bw"] },
"lstm2_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.1, "L2": 0.01, "direction": -1, "from" : ["lstm1_fw", "lstm1_bw"] },

"output" :   { "class" : "softmax", "loss" : "sprint", "ce_smoothing": 0.1, "loss_like_ce": True, "from" : ["lstm2_fw", "lstm2_bw"],
	"sprint_opts": {
		"sprintExecPath": "sprint/lattice-processor",
		"sprintConfigStr": "config:get_sprint_lattice_proc_args",
		"sprintControlConfig": {"verbose": True},
		"numInstances" : 4
	}}
}

seq_train_parallel = {"forward_seq_delay": 15}
testnet_share_params = True

# trainer
batching = "random"
batch_size = 5000
max_seqs = 50
max_seq_length = batch_size  # How exactly do we handle too long seqs?
chunking = "100"  # I guess we cannot use chunking.
truncation = -1
num_epochs = 30
model = "net-model/network"
gradient_clip = 10
rmsprop = 0.9 # this is a rho actually
momentum = 0.9
learning_rate = 0.0001
min_learning_rate = 0.000001
learning_rate_control = "newbob"
learning_rate_file = "newbob.data"
newbob_learning_rate_decay = 0.8
import_model_train_epoch1 = "/work/asr2/kulikov/setups-data/2016-06-04-chime3-cetrain/data-train/ce.lstm.3l.rmsprop.momentum.lr_1e-4/net-model/network.012"

# log
log = "log/train.crnn.%s.log" % task
log_verbosity = 5
